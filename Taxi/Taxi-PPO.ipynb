{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9e103e",
   "metadata": {},
   "source": [
    "# Taxi-PPO implementation\n",
    "\n",
    "PPO test in the Taxi environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d37ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import minerl\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c2074",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Class used to memorize the trajectory and calculate the advntage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b7a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    \n",
    "    STATE = 0\n",
    "    ACTION = 1\n",
    "    ACTION_PROB = 2\n",
    "    REWARD = 3\n",
    "    DONE = 4\n",
    "    \n",
    "    def __init__(self, n_trajectories, gamma = 0.4):\n",
    "        self.trajectories = np.empty(n_trajectories, dtype=object)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        \n",
    "    def collect(self, state, action, action_prob, reward, done, i_episode):\n",
    "        if (self.trajectories[i_episode] == None):\n",
    "            self.trajectories[i_episode] = deque(maxlen=N_STEPS)\n",
    "        self.trajectories[i_episode].append((state, action, action_prob, reward, done))\n",
    "        \n",
    "    def calculate_advantages(self):\n",
    "        self.advantages = []\n",
    "        self.TDerrors = [] #list of all the delta, used to uopdate the critic\n",
    "        \n",
    "        for trajectory in self.trajectories:\n",
    "            \n",
    "            advantage_trajectory = [] #list of advantages for each element in a single trajectory\n",
    "            delta = []\n",
    "            \n",
    "            old_advantage = trajectory[-1][self.REWARD]\n",
    "            delta.append(old_advantage)\n",
    "            advantage_trajectory.append(old_advantage)\n",
    "\n",
    "            for i in range(len(trajectory)-2,-1,-1):\n",
    "                delta.append(trajectory[i][self.REWARD] + self.gamma*ppo.return_v_values(trajectory[i+1][self.STATE]) - ppo.return_v_values(trajectory[i][self.STATE]))\n",
    "                new_advantage = delta[-1] + self.gamma*old_advantage\n",
    "                \n",
    "                advantage_trajectory.append(new_advantage[0])   \n",
    "                \n",
    "                old_advantage = new_advantage\n",
    "               \n",
    "            #reverse the list (at pos 0 there is the last advantage/delta)\n",
    "            advantage_trajectory = list(reversed(advantage_trajectory))\n",
    "            delta = list(reversed(delta))\n",
    "            \n",
    "            self.advantages.append(advantage_trajectory)\n",
    "            self.TDerrors.append(delta)\n",
    "            \n",
    "        #flat all trajectories in a single deque adding the advantages (easier to sample random batches)\n",
    "        self.flat_trajectories(self.trajectories, self.advantages, self.TDerrors)\n",
    "    \n",
    "    def flat_trajectories(self, trajectories, advantages, TDerrors):\n",
    "        \n",
    "        size = 0\n",
    "        for trajectory in trajectories:\n",
    "            size = size + len(trajectory)\n",
    "        \n",
    "        self.flatten_trajectories = deque(maxlen=size)\n",
    "        \n",
    "        for trajectory, advantage, delta in zip(trajectories, advantages, TDerrors):\n",
    "            for i in range(len(trajectory)):\n",
    "                self.flatten_trajectories.append((trajectory[i][self.STATE], trajectory[i][self.ACTION], trajectory[i][self.ACTION_PROB], trajectory[i][self.REWARD], advantage[i], delta[i], trajectory[i][self.DONE]))\n",
    "        \n",
    "        \n",
    "    #pick a random batch example from the flatten list of trajectories\n",
    "    def sample_experiences(self, batch_size):\n",
    "        if (len(self.flatten_trajectories) >= batch_size):\n",
    "            indices = np.random.permutation(len(self.flatten_trajectories))[:batch_size]\n",
    "        else:\n",
    "            indices = np.random.permutation(len(self.flatten_trajectories))\n",
    "        batch = [self.flatten_trajectories[index] for index in indices]\n",
    "        #delete form the memory the used obervations\n",
    "        for index in sorted(indices, reverse=True):\n",
    "            del self.flatten_trajectories[index]\n",
    "        states, actions, actions_prob, rewards, advantages, TDerrors, dones = [np.array([experience[field_index] for experience in batch]) for field_index in range(7)]\n",
    "        return states, actions, actions_prob, rewards, advantages, TDerrors, dones\n",
    "        \n",
    "    def reset(self):\n",
    "        for trajectory in self.trajectories:\n",
    "            trajectory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c35a3f8",
   "metadata": {},
   "source": [
    "# PPO class\n",
    "\n",
    "$ s_{t+1} $ is the observed state after the current action $ a_t $ \n",
    "\n",
    "Actor update formula:\n",
    "$ \\theta_{t+1} = \\theta_t + \\alpha\\nabla min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t)$\n",
    "\n",
    "Critic update formula:\n",
    "$ w_{t+1} = w_t + \\alpha\\delta_t\\nabla\\hat{v}(s_t,w)$\n",
    "\n",
    "Probability ratio $ r_t(\\theta) \\doteq $\n",
    "$ \\pi_\\theta(a_t | s_t) \\over \\pi_{\\theta_old}(a_t | s_t) $\n",
    "\n",
    "Advantage:\n",
    "$ \\hat{A}_t \\doteq \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + (\\gamma\\lambda)^2\\delta_{t+2} + ... + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1} = \\delta_t + (\\gamma\\lambda)\\hat{A}_{t+1}$\n",
    "\n",
    "TDerror:\n",
    "$ \\quad \\delta_t  \\doteq $\n",
    "$ r_t + \\gamma\\hat{v}(s_{t+1},w) - \\hat{v}(s_t,w) $ $ \\qquad $ (if $ s_{t+1} $ is terminal then $ \\hat{v}(s_{t+1},w) = 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b137fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(object):\n",
    "    \n",
    "    input_shape = [1] #500 states (400 reachable)\n",
    "    n_outputs = env.action_space.n # 6 actions\n",
    "    \n",
    "    def __init__(self, n_episodes = 1, train_steps = 100, epsilon = 0.2, alpha = 0.95):\n",
    "        self.actor = self.create_actor()\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        self.critic = self.create_critic()\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.train_steps = train_steps\n",
    "        \n",
    "        self.memory = Memory(n_episodes)\n",
    "        \n",
    "    #create the NN of the actor\n",
    "    # Given the state returns the probability of each action\n",
    "    def create_actor(self):    \n",
    "        actor = keras.Sequential([\n",
    "            keras.layers.Dense(32, activation=\"relu\", input_shape=self.input_shape),\n",
    "            keras.layers.Dense(32, activation=\"relu\"),\n",
    "            keras.layers.Dense(self.n_outputs, activation = 'softmax') ])\n",
    "        \n",
    "        return actor\n",
    "       \n",
    "    #create the NN of the critic\n",
    "    # Given the state returns the value function\n",
    "    def create_critic(self):\n",
    "        critic = keras.Sequential([\n",
    "            keras.layers.Dense(32, activation=\"relu\", input_shape=self.input_shape),\n",
    "            keras.layers.Dense(32, activation=\"relu\"),\n",
    "            keras.layers.Dense(1) ])\n",
    "        \n",
    "        self.critic_loss_fn = tf.keras.losses.mean_squared_error\n",
    "        \n",
    "        return critic\n",
    "      \n",
    "    def play_one_step(self, env, observation):\n",
    "        action, action_prob = self.select_action(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        self.memory.collect(observation, action, action_prob, reward, done, i_episode)\n",
    "        return observation, action, reward, done, info\n",
    "        \n",
    "    #select the action (returned as a number)\n",
    "    def select_action(self, observation):\n",
    "        \n",
    "        # explanation: tf.expand_dims(observation['pov'], axis=0)\n",
    "        # since we pass another input of shape (1,) -> we need to tell keras that is one image (it assumes the first dimension to be the batch)\n",
    "        \n",
    "        action_probabilities = self.actor.predict(np.array( [observation,])[np.newaxis, ...])[0]\n",
    "        \n",
    "        #choosing an action usign randomly using a \"roulette wheel\" approach\n",
    "        r = random.random()\n",
    "        \n",
    "        sum_probabilities = 0\n",
    "        for i in range(len(action_probabilities)):\n",
    "            sum_probabilities = sum_probabilities + action_probabilities[i]\n",
    "            \n",
    "            if (r <= sum_probabilities):\n",
    "                action = i\n",
    "                break\n",
    "        \n",
    "        return action, action_probabilities[action]\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        self.memory.calculate_advantages()\n",
    "        \n",
    "        for i_step in range(self.train_steps):\n",
    "            done = self.training_step(batch_size)\n",
    "            if (done):\n",
    "                break\n",
    "        \n",
    "        self.memory.reset()\n",
    "        \n",
    "    #training done on the memory (the advantages must be calculated before hand)\n",
    "    def training_step(self, batch_size):\n",
    "        #get experiences (parts of a trajectory) from the memory\n",
    "        experiences = self.memory.sample_experiences(batch_size)\n",
    "        states, actions, actions_prob, rewards, advantages, TDerrors, dones = experiences\n",
    "        \n",
    "        done = False\n",
    "        if (len(states) != batch_size):\n",
    "            done = True\n",
    "        \n",
    "        #compute the values for the update of the actor\n",
    "        \n",
    "        mask = tf.one_hot(actions, self.n_outputs)\n",
    "\n",
    "        #array of shape (64,) into array of shape (64,1)\n",
    "        states =  np.array(np.array_split(states, len(states)))\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            current_actions_prob = self.actor(states)\n",
    "            \n",
    "            current_action_prob = tf.reduce_sum(current_actions_prob*mask, axis=1, keepdims=True)\n",
    "            old_actions_prob = tf.reshape(tf.convert_to_tensor(actions_prob), [len(states), 1])\n",
    "            probability_ratio = tf.divide(current_action_prob, old_actions_prob)\n",
    "        \n",
    "            surrogate_arg_1 = tf.convert_to_tensor([probability_ratio[index]*advantages[index] for index in range(len(advantages))])\n",
    "            surrogate_arg_2 = tf.convert_to_tensor(np.array([tf.keras.backend.clip(probability_ratio,1-self.epsilon,1+self.epsilon)[index]*advantages[index] for index in range(len(advantages))]).flatten())\n",
    "            \n",
    "            L = 0 - tf.minimum( surrogate_arg_1 , surrogate_arg_2 ) \n",
    "            loss = tf.reduce_mean(L)\n",
    "\n",
    "        actor_weights = self.actor.trainable_variables\n",
    "        grads = tape.gradient(loss, actor_weights)\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, actor_weights))\n",
    "        \n",
    "        #update of the critic. We need the target is the TD error\n",
    "        target_v_values = tf.reshape(tf.convert_to_tensor(np.asarray(self.alpha*TDerrors).astype('float32')), (len(states), 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_v_values = self.critic(states)\n",
    "            v_values = tf.reduce_sum(all_v_values*mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(self.critic_loss_fn(target_v_values, v_values))\n",
    "        grads = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))\n",
    "        \n",
    "        #since v changed we need to re-calculate the advantages\n",
    "        #self.memory.calculate_advantages()\n",
    "        \n",
    "        return done\n",
    "    \n",
    "    def calculate_probability_ratio(self, action, observation):\n",
    "        one_hot_action = tf.one_hot(action, self.n_outputs)\n",
    "        \n",
    "        current_action_prob = tf.reduce_sum(self.actor.predict(observation)*one_hot_action, axis=1, keepdims=True)\n",
    "        old_action_prob = tf.reduce_sum(self.actor_old.predict(observation)*one_hot_action, axis=1, keepdims=True)\n",
    "    \n",
    "        return current_action_prob / old_action_prob\n",
    "    \n",
    "    def return_v_values(self, observation):\n",
    "        v = self.critic.predict(np.array( [observation,])[np.newaxis, ...])[0]\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802272f0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea6744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Episode 0 terminated after 199 steps with total reward: -200.0\n",
      "Epoch 0 Episode 1 terminated after 199 steps with total reward: -200.0\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "N_EPISODES = 3 # number of trajectories collected in one epoch\n",
    "N_STEPS = 200 # max number of step for each episode\n",
    "\n",
    "TRAIN_STEPS = 15 # number of max steps done during training. if the number of samples is less than TRAIN_STEPS*BATCH_SIZE will stop early after completing the training on all the samples\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "ppo = PPO(n_episodes = N_EPISODES, train_steps = TRAIN_STEPS)\n",
    "\n",
    "rewards = []\n",
    "\n",
    "\n",
    "for i_epoch in range(N_EPOCHS):\n",
    "    \n",
    "    epoch_reward = 0.0\n",
    "    \n",
    "    for i_episode in range(N_EPISODES):\n",
    "        \n",
    "        observation = env.reset()\n",
    "        \n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        for i_step in range(N_STEPS):   \n",
    "            \n",
    "            observation, action, reward, done, info = ppo.play_one_step(env, observation)\n",
    "\n",
    "            #env.render()\n",
    "            episode_reward = episode_reward + reward\n",
    "\n",
    "            if(done or i_step == N_STEPS-1):\n",
    "                print(\"Epoch {} Episode {} terminated after {} steps with total reward: {}\".format(i_epoch, i_episode, i_step, episode_reward))\n",
    "                epoch_reward = epoch_reward + episode_reward\n",
    "                break\n",
    "                \n",
    "    rewards.append(epoch_reward/N_EPISODES)\n",
    "    \n",
    "    ppo.train(batch_size = 64)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005d83d3",
   "metadata": {},
   "source": [
    "## Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82a0753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(N_EPOCHS)\n",
    "\n",
    "plt.plot(epochs, rewards)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e13133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
