{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b284cff8",
   "metadata": {},
   "source": [
    "# Atari-PPO-RND implementation\n",
    "\n",
    "PPO-RND test in the Atari environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d37ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing\n",
    "from gym.wrappers import FrameStack\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from collections import deque\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from threading import Thread\n",
    "import math\n",
    "\n",
    "# change keras setting to use the conv2d NN passing the channel first (as returned from the FrameStack wrapper)\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c963de",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Class used to memorize the trajectory and calculate the advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b7a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    \n",
    "    STATE = 0\n",
    "    ACTION = 1\n",
    "    ACTION_PROB = 2\n",
    "    EXTRINSIC_REWARD = 3\n",
    "    INTRINSIC_REWARD = 4\n",
    "    DONE = 5\n",
    "    \n",
    "    def __init__(self, n_trajectories, gamma = 0.4, e_lambda_par = 1, i_lambda_par = 1):\n",
    "        self.trajectories = np.empty(n_trajectories, dtype=object)\n",
    "        self.gamma = gamma\n",
    "        self.e_lambda_par = e_lambda_par\n",
    "        self.i_lambda_par = i_lambda_par\n",
    "              \n",
    "    def collect(self, state, action, action_prob, extrinsic_reward, intrinsic_reward, done, i_episode):\n",
    "        if (self.trajectories[i_episode] == None):\n",
    "            self.trajectories[i_episode] = deque(maxlen=N_STEPS)\n",
    "        self.trajectories[i_episode].append((state, action, action_prob, extrinsic_reward, intrinsic_reward, done))\n",
    "        \n",
    "    def calculate_advantages(self, reward_standard_deviation_estimate):\n",
    "        self.advantages = []\n",
    "        self.extrinsic_TDerrors = []\n",
    "        self.intrinsic_TDerrors = [] #list of all the delta, used to uopdate the critic\n",
    "        \n",
    "        for trajectory in self.trajectories:\n",
    "            \n",
    "            advantage_trajectory = [] #list of advantages for each element in a single trajectory\n",
    "            e_discounted_return = []\n",
    "            i_discounted_return = []\n",
    "\n",
    "            e_discounted_return.append(trajectory[-1][self.EXTRINSIC_REWARD])\n",
    "            e_old_advantage = trajectory[-1][self.EXTRINSIC_REWARD] - ppo.return_v_values(trajectory[-1][self.STATE]) \n",
    "            \n",
    "            #normalizing the intrinisc reward before calculating the advantage\n",
    "            i_discounted_return.append(trajectory[-1][self.INTRINSIC_REWARD] / reward_standard_deviation_estimate )\n",
    "            i_old_advantage = trajectory[-1][self.INTRINSIC_REWARD] / reward_standard_deviation_estimate - ppo.return_v_values(trajectory[-1][self.STATE]) \n",
    "            \n",
    "            advantage_trajectory.append(e_old_advantage + i_old_advantage)\n",
    "\n",
    "            for i in range(len(trajectory)-2,-1,-1):\n",
    "                e_discounted_return.append(trajectory[i][self.EXTRINSIC_REWARD] + self.gamma*ppo.return_v_extrinsic_values(trajectory[i+1][self.STATE]))\n",
    "                new_advantage = e_discounted_return[-1] - ppo.return_v_extrinsic_values(trajectory[-1][self.STATE]) + self.gamma*self.e_lambda_par*e_old_advantage\n",
    "                \n",
    "                e_old_advantage = new_advantage\n",
    "                \n",
    "                normalized_intrinsic_reward = trajectory[-1][self.INTRINSIC_REWARD] / reward_standard_deviation_estimate\n",
    "                i_discounted_return.append(normalized_intrinsic_reward + self.gamma*ppo.return_v_intrinsic_values(trajectory[i+1][self.STATE]))\n",
    "                new_advantage = i_discounted_return[-1] - ppo.return_v_intrinsic_values(trajectory[-1][self.STATE]) + self.gamma*self.i_lambda_par*i_old_advantage\n",
    "                \n",
    "                i_old_advantage = new_advantage\n",
    "                \n",
    "                advantage_trajectory.append(i_old_advantage[0] + e_old_advantage[0])  \n",
    "        \n",
    "            self.extrinsic_TDerrors.append(e_discounted_return)\n",
    "            self.intrinsic_TDerrors.append(i_discounted_return)\n",
    "            \n",
    "            self.advantages.append(advantage_trajectory)\n",
    "            \n",
    "        #flat all trajectories in a single deque adding the advantages (easier to sample random batches)\n",
    "        self.flat_trajectories(self.trajectories, self.advantages, self.extrinsic_TDerrors, self.intrinsic_TDerrors)\n",
    "    \n",
    "    def flat_trajectories(self, trajectories, advantages, extrinsic_TDerrors, intrinsic_TDerrors):\n",
    "        \n",
    "        size = 0\n",
    "        for trajectory in trajectories:\n",
    "            size = size + len(trajectory)\n",
    "        \n",
    "        self.flatten_trajectories = deque(maxlen=size)\n",
    "        \n",
    "        for trajectory, advantage, e_delta, i_delta in zip(trajectories, advantages, extrinsic_TDerrors, intrinsic_TDerrors):\n",
    "            for i in range(len(trajectory)-1,-1,-1):\n",
    "                self.flatten_trajectories.append((trajectory[i][self.STATE], trajectory[i][self.ACTION], trajectory[i][self.ACTION_PROB], trajectory[i][self.EXTRINSIC_REWARD], trajectory[i][self.INTRINSIC_REWARD], advantage[i], e_delta[i], i_delta[i], trajectory[i][self.DONE]))\n",
    "        \n",
    "        \n",
    "    #pick a random batch example from the flatten list of trajectories\n",
    "    def sample_experiences(self, batch_size):\n",
    "        if (len(self.flatten_trajectories) >= batch_size):\n",
    "            indices = np.random.permutation(len(self.flatten_trajectories))[:batch_size]\n",
    "        else:\n",
    "            indices = np.random.permutation(len(self.flatten_trajectories))\n",
    "        batch = [self.flatten_trajectories[index] for index in indices]\n",
    "        #delete form the memory the used obervations\n",
    "        for index in sorted(indices, reverse=True):\n",
    "            del self.flatten_trajectories[index]\n",
    "        states, actions, actions_prob, e_rewards, i_rewards, advantages, e_TDerrors, i_TDerrors, dones = [np.array([experience[field_index] for experience in batch]) for field_index in range(9)]\n",
    "        return states, actions, actions_prob, e_rewards, i_rewards, advantages, e_TDerrors, i_TDerrors, dones\n",
    "        \n",
    "    def reset(self):\n",
    "        for trajectory in self.trajectories:\n",
    "            trajectory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e6bd4",
   "metadata": {},
   "source": [
    "# RND class\n",
    "\n",
    "$ s_{t+1} $ is the observed state after the current action $ a_t $ \n",
    "\n",
    "Predictor update:\n",
    "minimize $ \\hat{f}(x, \\theta) - f(x) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b20718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RND(object):\n",
    "    \n",
    "    input_shape = [1,84,84] \n",
    "    n_outputs = 200\n",
    "    \n",
    "    N_intrinsic_rewards = 0 #number of intrinsic reward received\n",
    "    intrinisc_reward_mean = 0.0 #mean of the intrinsic rewards received\n",
    "    reward_M2 = 0.0 #sum of squares of differences from the current mean\n",
    "    \n",
    "    N_observations = 0 #number of observations received\n",
    "    observations_mean = 0.0 #mean of the observations received\n",
    "    observation_M2 = 0.0 #sum of squares of differences from the current mean\n",
    "    \n",
    "    def __init__(self, env, n_normalization_steps = 40):\n",
    "        self.target = self.create_target()\n",
    "        self.predictor = self.create_predictor()\n",
    "        \n",
    "        self.MSE = tf.keras.losses.mean_squared_error\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        self.initialize_standard_deviation_estimate(env, n_normalization_steps)\n",
    "        \n",
    "    #create the NN of the target\n",
    "    def create_target(self):\n",
    "        target = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(self.n_outputs) ])\n",
    "        return target\n",
    "        \n",
    "    #create the NN of the predictor\n",
    "    def create_predictor(self):\n",
    "        predictor = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(self.n_outputs) ])\n",
    "        return predictor\n",
    "    \n",
    "    def train_predictor(self, observations):\n",
    "        # extrinsic critic (rewards from the envirnoment)\n",
    "        observations = np.array(observations)\n",
    "        observations = self.normalize_observations(observations)\n",
    "        # covert shape [BATCH_SIZE, 4, 84, 84] in [BATCH_SIZE,1,84,84]\n",
    "        observations = [observation[-1,0:observation.shape[1], 0:observation.shape[2]] for observation in observations]\n",
    "        observations = tf.expand_dims(observations, axis = 1)\n",
    "        target_values = self.target.predict(observations)\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_values = self.predictor(observations)\n",
    "            loss = tf.reduce_mean(self.MSE(target_values, all_values))\n",
    "        grads = tape.gradient(loss, self.predictor.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.predictor.trainable_variables))\n",
    "        \n",
    "    def calculate_intrinsic_reward(self, observation):\n",
    "        #passing a (4,84,84) stacked frame from the environment\n",
    "        observation = np.array(observation)\n",
    "        #picking the last frame\n",
    "        observation = observation[-1, 0:observation.shape[1], 0:observation.shape[2]]\n",
    "        #normalize the last frame\n",
    "        s = self.calculate_observation_standard_deviation()\n",
    "        observation = self.normalize_observation(observation, s)\n",
    "        #calculate intrinsic reward on the last frame\n",
    "        observation = tf.expand_dims(observation, axis=0)\n",
    "        f_target = self.target.predict(tf.expand_dims(observation, axis=0))\n",
    "        f_predictor = self.predictor.predict(tf.expand_dims(observation, axis=0))\n",
    "        #return self.MSE(f_target, f_predictor)*10000\n",
    "        return pow(np.linalg.norm(f_predictor - f_target), 2)*100\n",
    "    \n",
    "    def initialize_standard_deviation_estimate(self, env, n_normalization_steps):\n",
    "        observations_mean = np.zeros(env.observation_space.shape ,'float64') #mean of the intrinsic rewards received\n",
    "        observation_M2 = np.zeros(env.observation_space.shape ,'float64') #sum of squares of differences from the current mean\n",
    "        \n",
    "        obsevation = env.reset()\n",
    "        \n",
    "        for i_step in range(n_normalization_steps):\n",
    "            random_action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(random_action)\n",
    "            for frame in observation:\n",
    "                self.update_observation_normalization_param(frame)\n",
    "    \n",
    "    def update_observation_normalization_param(self, observation):\n",
    "        #cicle trhough the 4 images that makes up for an observation\n",
    "        for obs in observation:\n",
    "            #obs_mean = np.mean([obs_dim], axis=0)\n",
    "            self.N_observations = self.N_observations + 1\n",
    "            delta = obs - self.observations_mean\n",
    "            self.observations_mean = self.observations_mean + delta/self.N_observations # mean_N = mean_{N-1} + (obs_t - mean_{N-1}) / N\n",
    "            self.observation_M2 = self.observation_M2 + delta*(obs - self.observations_mean)\n",
    "        \n",
    "    def calculate_observation_standard_deviation(self):\n",
    "        standard_deviation = np.sqrt( self.observation_M2 / (self.N_observations - 1))\n",
    "        return standard_deviation\n",
    "    \n",
    "    def normalize_observations(self, observations):\n",
    "\n",
    "        norm_obs = []\n",
    "        s = self.calculate_observation_standard_deviation()\n",
    "        for observation in observations:\n",
    "            norm_obs.append(self.normalize_observation(observation, s))\n",
    "        normalized_obs = tf.stack([norm_obs[i] for i in range(len(norm_obs))], 0)\n",
    "\n",
    "        return normalized_obs\n",
    "    \n",
    "    def normalize_observation(self, observation, standard_deviation):\n",
    "        t = observation - self.observations_mean\n",
    "        normalized_obs = np.clip(np.divide(t, standard_deviation, out=np.zeros_like(t), where=standard_deviation!=0), a_min =-5, a_max = 5)       \n",
    "        return normalized_obs\n",
    "    \n",
    "    #def normalize_observation(self, observation, standard_deviation):\n",
    "    #    temp_obs = []\n",
    "    #    for obs_dim in observation:\n",
    "    #        t = obs_dim - self.observations_mean\n",
    "    #        temp_obs.append(np.clip(np.divide(t, standard_deviation, out=np.zeros_like(t), where=standard_deviation!=0), a_min =-5, a_max = 5))\n",
    "    #    normalized_obs = tf.squeeze(tf.stack([temp_obs[i] for i in range(len(temp_obs))], axis = 0))\n",
    "    #    return normalized_obs\n",
    "    \n",
    "    #Using welford's algorithm\n",
    "    def update_reward_normalization_param(self, i_reward):\n",
    "        self.N_intrinsic_rewards = self.N_intrinsic_rewards + 1\n",
    "        delta = i_reward - self.intrinisc_reward_mean\n",
    "        self.intrinisc_reward_mean = self.intrinisc_reward_mean + delta/self.N_intrinsic_rewards # mean_N = mean_{N-1} + (i_t - mean_{N-1}) / N\n",
    "        self.reward_M2 = self.reward_M2 + delta*(i_reward - self.intrinisc_reward_mean)\n",
    "        \n",
    "    def calculate_reward_standard_deviation(self):\n",
    "        standard_deviation = math.sqrt( self.reward_M2 / (self.N_intrinsic_rewards - 1))\n",
    "        print(\"===============================================================\")\n",
    "        print(\"===============================================================\")\n",
    "        print(\"STANDARD DEVIATION {}\".format(standard_deviation))\n",
    "        print(\"===============================================================\")\n",
    "        print(\"===============================================================\")\n",
    "        return standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2077a580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#basic_env = gym.make(\"GravitarNoFrameskip-v4\", obs_type = \"image\")\n",
    "#wrapped_env = AtariPreprocessing(basic_env)\n",
    "#env = FrameStack(wrapped_env, 4)\n",
    "\n",
    "#rnd = RND(env)\n",
    "\n",
    "#env.reset()\n",
    "\n",
    "#obs = []\n",
    "#for _ in range(32):\n",
    "#    random_action = env.action_space.sample()\n",
    "#    new_obs, reward, done, info = env.step(random_action)\n",
    "#    obs.append(new_obs)\n",
    "    \n",
    "#array = np.array(obs)\n",
    "#rnd.train_predictor(array)\n",
    "\n",
    "#i = rnd.calculate_intrinsic_reward(new_obs)\n",
    "\n",
    "#print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875cd80",
   "metadata": {},
   "source": [
    "# PPO class\n",
    "\n",
    "$ s_{t+1} $ is the observed state after the current action $ a_t $ \n",
    "\n",
    "Actor update formula:\n",
    "$ \\theta_{t+1} = \\theta_t + \\alpha\\nabla min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t)$\n",
    "\n",
    "Critic update formula:\n",
    "$ w_{t+1} = w_t + \\alpha\\delta_t\\nabla\\hat{v}(s_t,w)$\n",
    "\n",
    "Probability ratio $ r_t(\\theta) \\doteq $\n",
    "$ \\pi_\\theta(a_t | s_t) \\over \\pi_{\\theta_{old}}(a_t | s_t) $\n",
    "\n",
    "Advantage:\n",
    "$ \\hat{A}_t \\doteq \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + (\\gamma\\lambda)^2\\delta_{t+2} + ... + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1} = \\delta_t + (\\gamma\\lambda)\\hat{A}_{t+1}$\n",
    "\n",
    "TDerror:\n",
    "$ \\quad \\delta_t  \\doteq $\n",
    "$ r_t + \\gamma\\hat{v}(s_{t+1},w) - \\hat{v}(s_t,w) $ $ \\qquad $ (if $ s_{t+1} $ is terminal then $ \\hat{v}(s_{t+1},w) = 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62b137fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(object):\n",
    "    \n",
    "    input_shape = [4,84,84] \n",
    "    #n_outputs = 6 #wrapped_env.action_space.n\n",
    "    \n",
    "    def __init__(self, env, n_episodes = 1, train_steps = 100, epsilon = 0.2, alpha = 0.95, gamma = 0.4, e_lambda_par = 1, i_lambda_par = 1, n_normalization_steps = 300, train_predictor_keeping_prob = 0.25):\n",
    "        self.actor = self.create_actor()\n",
    "        self.intrinsic_critic = self.create_critic()\n",
    "        self.extrinsic_critic = self.create_critic()\n",
    "        \n",
    "        self.MSE = tf.keras.losses.mean_squared_error\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.n_outputs = env.action_space.n\n",
    "        \n",
    "        self.train_steps = train_steps\n",
    "        self.train_predictor_keeping_prob = train_predictor_keeping_prob\n",
    "        \n",
    "        self.memory = Memory(n_episodes, gamma, e_lambda_par, i_lambda_par)\n",
    "        \n",
    "        self.rnd = RND(env, n_normalization_steps)\n",
    "        \n",
    "    #create the NN of the actor\n",
    "    # Given the state returns the probability of each action\n",
    "    def create_actor(self):    \n",
    "        actor = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(self.n_outputs, activation = 'softmax') ])\n",
    "        return actor\n",
    "       \n",
    "    #create the NN of the critic\n",
    "    # Given the state returns the value function\n",
    "    def create_critic(self):\n",
    "        critic = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(1) ])\n",
    "        \n",
    "        return critic\n",
    "      \n",
    "    def play_one_step(self, env, observation, i_episode):\n",
    "        action, action_prob = self.select_action(observation)\n",
    "        observation, e_reward, done, info = env.step(action)\n",
    "        \n",
    "        #put in wrapper\n",
    "        e_reward = float(e_reward)/100.\n",
    "        \n",
    "        #the normalization of the intrinisc reward after before training\n",
    "        i_reward = self.rnd.calculate_intrinsic_reward(observation)\n",
    "        \n",
    "        self.rnd.update_reward_normalization_param(i_reward)\n",
    "        self.rnd.update_observation_normalization_param(observation)\n",
    "        \n",
    "        self.memory.collect(observation, action, action_prob, e_reward, i_reward, done, i_episode)\n",
    "        \n",
    "        return observation, action, e_reward, i_reward, done, info\n",
    "        \n",
    "    #select the action (returned as a number)\n",
    "    def select_action(self, observation):\n",
    "        \n",
    "        # explanation: tf.expand_dims(observation['pov'], axis=0)\n",
    "        # since we pass another input of shape (1,) -> we need to tell keras that is one image (it assumes the first dimension to be the batch)\n",
    "        action_probabilities = self.actor.predict(tf.expand_dims(observation, axis=0))[0]\n",
    "        \n",
    "        #choosing an action usign randomly using a \"roulette wheel\" approach\n",
    "        r = random.random()\n",
    "        \n",
    "        sum_probabilities = 0\n",
    "        for i in range(len(action_probabilities)):\n",
    "            sum_probabilities = sum_probabilities + action_probabilities[i]\n",
    "            \n",
    "            if (r <= sum_probabilities):\n",
    "                action = i\n",
    "                break\n",
    "        \n",
    "        return action, action_probabilities[action]\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        self.memory.calculate_advantages(self.rnd.calculate_reward_standard_deviation())\n",
    "        \n",
    "        for i_step in range(self.train_steps):\n",
    "            done = self.training_step(batch_size)\n",
    "            if (done):\n",
    "                break\n",
    "        \n",
    "        self.memory.reset()\n",
    "        \n",
    "    #training done on the memory (the advantages must be calculated before hand)\n",
    "    def training_step(self, batch_size):\n",
    "        #get experiences (parts of a trajectory) from the memory\n",
    "        experiences = self.memory.sample_experiences(batch_size)\n",
    "        \n",
    "        states, actions, actions_prob, extrinsic_rewards, intrinsic_rewards, advantages, extrinsic_TDerrors, intrinsic_TDerrors, dones = experiences\n",
    "        \n",
    "        done = False\n",
    "        if (len(states) != batch_size):\n",
    "            done = True\n",
    "        \n",
    "        #compute the values for the update of the actor\n",
    "        \n",
    "        mask = tf.one_hot(actions, self.n_outputs)\n",
    "        \n",
    "        states = np.array(states)\n",
    "        \n",
    "        og_states = states\n",
    "        states = states/255\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            current_actions_prob = self.actor(states)\n",
    "            \n",
    "            current_action_prob = tf.reduce_sum(current_actions_prob*mask, axis=1, keepdims=True)\n",
    "            old_actions_prob = tf.reshape(tf.convert_to_tensor(actions_prob), [len(states), 1])\n",
    "            probability_ratio = tf.divide(tf.math.log(current_action_prob), tf.math.log(old_actions_prob))\n",
    "        \n",
    "            surrogate_arg_1 = tf.convert_to_tensor([probability_ratio[index]*advantages[index] for index in range(len(advantages))])\n",
    "            surrogate_arg_2 = tf.convert_to_tensor(np.array([tf.keras.backend.clip(probability_ratio,1-self.epsilon,1+self.epsilon)[index]*advantages[index] for index in range(len(advantages))]).flatten())\n",
    "            \n",
    "            L = 0 - tf.minimum( surrogate_arg_1 , surrogate_arg_2 ) \n",
    "            loss = tf.reduce_mean(L)\n",
    "\n",
    "        actor_weights = self.actor.trainable_variables\n",
    "        grads = tape.gradient(loss, actor_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, actor_weights))\n",
    "        \n",
    "        #update of the critic. The target is the TD error\n",
    "        \n",
    "        # extrinsic critic (rewards from the envirnoment)\n",
    "        target_v_values = tf.reshape(tf.convert_to_tensor(np.asarray(self.alpha*extrinsic_TDerrors).astype('float32')), (len(states), 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_v_values = self.extrinsic_critic(states)\n",
    "            v_values = tf.reduce_sum(all_v_values*mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(self.MSE(target_v_values, v_values))\n",
    "        grads = tape.gradient(loss, self.extrinsic_critic.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.extrinsic_critic.trainable_variables))\n",
    "        \n",
    "        # intrinsic critic (rewards from the exploration)\n",
    "        target_v_values = tf.reshape(tf.convert_to_tensor(np.asarray(self.alpha*intrinsic_TDerrors).astype('float32')), (len(states), 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_v_values = self.intrinsic_critic(states)\n",
    "            v_values = tf.reduce_sum(all_v_values*mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(self.MSE(target_v_values, v_values))\n",
    "        grads = tape.gradient(loss, self.intrinsic_critic.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.intrinsic_critic.trainable_variables))\n",
    "        \n",
    "        #since v changed we need to re-calculate the advantages\n",
    "        #self.memory.calculate_advantages()\n",
    "        \n",
    "        keep = random.random()\n",
    "        if (keep <= self.train_predictor_keeping_prob):\n",
    "            self.rnd.train_predictor(og_states)\n",
    "        \n",
    "        return done\n",
    "    \n",
    "    def return_v_extrinsic_values(self, observation):\n",
    "        v_e = self.extrinsic_critic.predict(tf.expand_dims(observation, axis=0))[0]\n",
    "        return v_e\n",
    "    \n",
    "    def return_v_intrinsic_values(self, observation):\n",
    "        v_i = self.intrinsic_critic.predict(tf.expand_dims(observation, axis=0))[0]\n",
    "        return v_i\n",
    "    \n",
    "    def save():\n",
    "        self.actor.save_weights(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2869d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45a4f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class collect_trajectory(Thread):\n",
    "    \n",
    "    def __init__(self, environment, i_agent ):\n",
    "         \n",
    "        Thread.__init__(self)   \n",
    "        self.n_agent = i_agent\n",
    "        self.rewards = [] \n",
    "        \n",
    "        self.basic_env = gym.make(environment, obs_type = \"image\")\n",
    "        self.wrapped_env = AtariPreprocessing(self.basic_env)\n",
    "        self.stack_env = FrameStack(self.wrapped_env, 4)\n",
    "            \n",
    "    def run(self):\n",
    "        print(\"Starting {}\".format(self.n_agent))\n",
    "        \n",
    "        observation = self.stack_env.reset()\n",
    "        \n",
    "        extrinsic_episode_reward = 0.0\n",
    "        intrinsic_episode_reward = 0.0\n",
    "        self.n_episodes = 0.0\n",
    "        self.extrinsic_tot_reward = 0.0\n",
    "        self.intrinsic_tot_reward = 0.0\n",
    "        \n",
    "        for i_step in range(N_STEPS):   \n",
    "            observation, action, extrinsic_reward, intrinsic_reward, done, info = ppo.play_one_step(self.stack_env, observation, self.n_agent)\n",
    "\n",
    "            #wrapped_env.render()\n",
    "            extrinsic_episode_reward = extrinsic_episode_reward + extrinsic_reward\n",
    "            intrinsic_episode_reward = intrinsic_episode_reward + intrinsic_reward\n",
    "            \n",
    "            #continuing task. if an episode is done we continue until complting the number of steps\n",
    "            if (done):\n",
    "                observation = self.stack_env.reset()\n",
    "                \n",
    "                self.extrinsic_tot_reward = self.extrinsic_tot_reward + extrinsic_episode_reward\n",
    "                self.intrinsic_tot_reward = self.intrinsic_tot_reward + intrinsic_episode_reward\n",
    "                self.n_episodes = self.n_episodes + 1\n",
    "                episode_reward = 0.0\n",
    "                \n",
    "        if (self.n_episodes == 0):\n",
    "            self.extrinsic_tot_reward = extrinsic_episode_reward\n",
    "            self.intrinsic_tot_reward = intrinsic_episode_reward\n",
    "        \n",
    "        self.stack_env.close()\n",
    "        \n",
    "        if (self.n_episodes > 0):\n",
    "            print(\"Exiting {} after {} episodes. Average ex reward: {} in reward: {}\".format(self.n_agent, self.n_episodes, self.extrinsic_tot_reward/self.n_episodes, self.intrinsic_tot_reward/self.n_episodes))\n",
    "        else:\n",
    "            print(\"Exiting {} after {} episodes. Average ex reward: {} in reward: {}\".format(self.n_agent, self.n_episodes, self.extrinsic_tot_reward, self.intrinsic_tot_reward))\n",
    "    \n",
    "    \n",
    "    def get_reward_average(self):\n",
    "        if (self.n_episodes > 0):\n",
    "            return (self.extrinsic_tot_reward/self.n_episodes, self.intrinsic_tot_reward/self.n_episodes)\n",
    "        else:\n",
    "            return (self.extrinsic_tot_reward, self.intrinsic_tot_reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700dfc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0\n",
      "Starting 1\n",
      "Starting 2\n",
      "Starting 3\n",
      "Exiting 1 after 2.0 episodes. Average ex reward: 0.0 in reward: 6751.863908497523\n",
      "Exiting 2 after 2.0 episodes. Average ex reward: 0.0 in reward: 24920.10628739869\n",
      "Exiting 3 after 3.0 episodes. Average ex reward: 0.0 in reward: 13684.755940298253\n",
      "Exiting 0 after 2.0 episodes. Average ex reward: 3.5 in reward: 7975.449099376853\n",
      "Epoch: 0 ended with average extrinsic reward: 0.875 intrinsic reward 13333.04380889283 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 11.512907438602369\n",
      "===============================================================\n",
      "===============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paci3\\AppData\\Local\\Temp\\ipykernel_15428\\2477841587.py:90: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  states, actions, actions_prob, e_rewards, i_rewards, advantages, e_TDerrors, i_TDerrors, dones = [np.array([experience[field_index] for experience in batch]) for field_index in range(9)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0\n",
      "Starting 1\n",
      "Starting 2\n",
      "Starting 3\n",
      "Exiting 0 after 4.0 episodes. Average ex reward: 0.0 in reward: 109.57432681615715\n",
      "Exiting 1 after 4.0 episodes. Average ex reward: 0.0 in reward: 108.12579255951286\n",
      "Exiting 3 after 4.0 episodes. Average ex reward: 0.0 in reward: 112.74862643345311\n",
      "Exiting 2 after 4.0 episodes. Average ex reward: 0.0 in reward: 107.91741626214282\n",
      "Epoch: 1 ended with average extrinsic reward: 0.0 intrinsic reward 109.59154051781648 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 8.932455337385417\n",
      "===============================================================\n",
      "===============================================================\n",
      "Starting 0\n",
      "Starting 1\n",
      "Starting 2\n",
      "Starting 3\n",
      "Exiting 1 after 5.0 episodes. Average ex reward: 0.0 in reward: 92.47033072950268\n",
      "Exiting 2 after 5.0 episodes. Average ex reward: 0.0 in reward: 93.23142719484608\n",
      "Exiting 0 after 5.0 episodes. Average ex reward: 0.0 in reward: 96.41312040173348\n",
      "Exiting 3 after 5.0 episodes. Average ex reward: 0.0 in reward: 93.19296293324082\n",
      "Epoch: 2 ended with average extrinsic reward: 0.0 intrinsic reward 93.82696031483077 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 7.499751789039346\n",
      "===============================================================\n",
      "===============================================================\n",
      "Starting 0\n",
      "Starting 1\n",
      "Starting 2\n",
      "Starting 3\n",
      "Exiting 1 after 4.0 episodes. Average ex reward: 0.0 in reward: 70.25042551378942\n",
      "Exiting 0 after 4.0 episodes. Average ex reward: 0.0 in reward: 67.41623335333031\n",
      "Exiting 3 after 4.0 episodes. Average ex reward: 0.0 in reward: 69.62335599967524\n",
      "Exiting 2 after 4.0 episodes. Average ex reward: 0.0 in reward: 69.22430327071717\n",
      "Epoch: 3 ended with average extrinsic reward: 0.0 intrinsic reward 69.12857953437803 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 6.583409552653334\n",
      "===============================================================\n",
      "===============================================================\n",
      "Starting 0\n",
      "Starting 1\n",
      "Starting 2\n",
      "Starting 3\n",
      "Exiting 0 after 4.0 episodes. Average ex reward: 0.0 in reward: 43.521763573668174\n",
      "Exiting 1 after 4.0 episodes. Average ex reward: 0.0 in reward: 44.090269656296336\n",
      "Exiting 3 after 4.0 episodes. Average ex reward: 0.0 in reward: 43.71764978626315\n",
      "Exiting 2 after 4.0 episodes. Average ex reward: 0.0 in reward: 44.955193703149924\n",
      "Epoch: 4 ended with average extrinsic reward: 0.0 intrinsic reward 44.07121917984439 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 5.936324769195526\n",
      "===============================================================\n",
      "===============================================================\n",
      "Starting 0\n",
      "Starting 1\n",
      "Starting 2\n",
      "Starting 3\n",
      "Exiting 0 after 4.0 episodes. Average ex reward: 0.0 in reward: 38.62857711839428\n",
      "Exiting 3 after 4.0 episodes. Average ex reward: 0.0 in reward: 37.29191047577498\n",
      "Exiting 1 after 4.0 episodes. Average ex reward: 0.0 in reward: 38.36206731964173\n",
      "Exiting 2 after 4.0 episodes. Average ex reward: 0.0 in reward: 38.62680361901143\n",
      "Epoch: 5 ended with average extrinsic reward: 0.0 intrinsic reward 38.22733963320561 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 5.448257689667565\n",
      "===============================================================\n",
      "===============================================================\n",
      "Starting 0\n",
      "Starting 1\n",
      "Starting 2\n",
      "Starting 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-29:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Programmi\\Anaconda\\envs\\gputest\\lib\\threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\paci3\\AppData\\Local\\Temp\\ipykernel_15428\\3921193840.py\", line 26, in run\n",
      "  File \"C:\\Users\\paci3\\AppData\\Local\\Temp\\ipykernel_15428\\2878981515.py\", line 59, in play_one_step\n",
      "  File \"C:\\Users\\paci3\\AppData\\Local\\Temp\\ipykernel_15428\\4173246151.py\", line 69, in calculate_intrinsic_reward\n",
      "  File \"D:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py\", line 1743, in predict\n",
      "    self.predict_function = self.make_predict_function()\n",
      "  File \"D:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py\", line 1562, in make_predict_function\n",
      "    if self.predict_function is not None and not force:\n",
      "AttributeError: 'Sequential' object has no attribute 'predict_function'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting 1 after 4.0 episodes. Average ex reward: 0.0 in reward: 34.033111564515934\n",
      "Exiting 3 after 4.0 episodes. Average ex reward: 0.0 in reward: 34.741937894220506\n",
      "Exiting 2 after 4.0 episodes. Average ex reward: 0.0 in reward: 34.32482956564657\n",
      "Epoch: 6 ended with average extrinsic reward: 0.0 intrinsic reward 32.6825144003928 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 5.085691970539334\n",
      "===============================================================\n",
      "===============================================================\n",
      "Starting 0\n",
      "Starting 1\n",
      "Starting 2\n",
      "Starting 3\n",
      "Exiting 1 after 4.0 episodes. Average ex reward: 0.0 in reward: 30.096235074386993\n",
      "Exiting 2 after 4.0 episodes. Average ex reward: 0.0 in reward: 30.27078480284588\n",
      "Exiting 3 after 4.0 episodes. Average ex reward: 0.0 in reward: 30.330081949532946\n",
      "Exiting 0 after 4.0 episodes. Average ex reward: 0.0 in reward: 30.44978568809474\n",
      "Epoch: 7 ended with average extrinsic reward: 0.0 intrinsic reward 30.28672187871514 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 4.768347967911577\n",
      "===============================================================\n",
      "===============================================================\n",
      "Starting 0\n",
      "Starting 1\n",
      "Starting 2\n",
      "Starting 3\n",
      "Exiting 3 after 4.0 episodes. Average ex reward: 0.0 in reward: 28.451304543394738\n",
      "Exiting 0 after 4.0 episodes. Average ex reward: 0.0 in reward: 28.15451990056711\n",
      "Exiting 2 after 4.0 episodes. Average ex reward: 0.0 in reward: 28.414467980589656\n",
      "Exiting 1 after 4.0 episodes. Average ex reward: 0.0 in reward: 27.997590218646685\n",
      "Epoch: 8 ended with average extrinsic reward: 0.0 intrinsic reward 28.254470660799548 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 4.5037765179903335\n",
      "===============================================================\n",
      "===============================================================\n",
      "Starting 0\n",
      "Starting 1\n",
      "Starting 2\n",
      "Starting 3\n",
      "Exiting 0 after 4.0 episodes. Average ex reward: 0.0 in reward: 25.68109807666627\n",
      "Exiting 1 after 4.0 episodes. Average ex reward: 0.0 in reward: 25.81984747938592\n",
      "Exiting 3 after 4.0 episodes. Average ex reward: 0.0 in reward: 25.747481089996917\n",
      "Exiting 2 after 4.0 episodes. Average ex reward: 0.0 in reward: 25.901105124326676\n",
      "Epoch: 9 ended with average extrinsic reward: 0.0 intrinsic reward 25.787382942593943 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 4.278836585098401\n",
      "===============================================================\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "N_STEPS_NORMALIZATION = 50\n",
    "N_EPOCHS = 100\n",
    "N_EPISODES = 4 # in multi-agent this is the number of agents (each agnet collect 1 trajectory)\n",
    "N_STEPS = 2400 # max number of step for each episode\n",
    "\n",
    "TRAIN_STEPS = 150 # number of max steps done during training. if the number of samples is less than TRAIN_STEPS*BATCH_SIZE will stop early after completing the training on all the samples\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "environment = \"PongNoFrameskip-v4\" # \"GravitarNoFrameskip-v4\", \"AirRaidNoFrameskip-v4\"\n",
    "\n",
    "#env used to initialize the parameters inside PPO and RND\n",
    "norm_env = gym.make(environment, obs_type = \"image\")\n",
    "norm_wrapped_env = AtariPreprocessing(norm_env)\n",
    "norm_stack_env = FrameStack(norm_wrapped_env, 4)\n",
    "\n",
    "ppo = PPO(norm_stack_env, n_episodes = N_EPISODES, train_steps = TRAIN_STEPS, n_normalization_steps = N_STEPS_NORMALIZATION, i_lambda_par = 2.25, train_predictor_keeping_prob = 0.65)\n",
    "\n",
    "e_rewards = []\n",
    "i_rewards = []\n",
    "\n",
    "for i_epoch in range(N_EPOCHS):\n",
    "    extrinsic_epoch_reward = 0.0\n",
    "    intrinsic_epoch_reward = 0.0\n",
    "    agents = []\n",
    "    for i_agent in range(N_EPISODES):\n",
    "        agents.append(collect_trajectory(environment = environment, i_agent = i_agent))\n",
    "    for agent in agents:\n",
    "        agent.start()\n",
    "    for agent in agents:\n",
    "        agent.join()\n",
    "        extrinsic_reward_average, intrinsic_reward_average = agent.get_reward_average()\n",
    "        extrinsic_epoch_reward = extrinsic_epoch_reward + extrinsic_reward_average\n",
    "        intrinsic_epoch_reward = intrinsic_epoch_reward + intrinsic_reward_average\n",
    "    e_rewards.append(extrinsic_epoch_reward/N_EPISODES)\n",
    "    i_rewards.append(intrinsic_epoch_reward/N_EPISODES)\n",
    "    print(\"Epoch: {} ended with average extrinsic reward: {} intrinsic reward {} \\n\".format(i_epoch, extrinsic_epoch_reward/N_EPISODES, intrinsic_epoch_reward/N_EPISODES) )  \n",
    "    ppo.train(batch_size = BATCH_SIZE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e7093c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"GravitarNoFrameskip-v4\", full_action_space = False) #, obs_type = \"image\")\n",
    "\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5b122",
   "metadata": {},
   "source": [
    "## Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(205)#N_EPOCHS)\n",
    "\n",
    "plt.plot(epochs, i_rewards)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d710415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
