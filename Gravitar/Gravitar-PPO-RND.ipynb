{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b284cff8",
   "metadata": {},
   "source": [
    "# Graviatar-PPO-RND implementation\n",
    "\n",
    "PPO-RND test in the Gravitar environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d37ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from threading import Thread\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c963de",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Class used to memorize the trajectory and calculate the advntage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b7a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    \n",
    "    STATE = 0\n",
    "    ACTION = 1\n",
    "    ACTION_PROB = 2\n",
    "    EXTRINSIC_REWARD = 3\n",
    "    INTRINSIC_REWARD = 4\n",
    "    DONE = 5\n",
    "    \n",
    "    def __init__(self, n_trajectories, gamma = 0.4):\n",
    "        self.trajectories = np.empty(n_trajectories, dtype=object)\n",
    "        self.gamma = gamma\n",
    "              \n",
    "    def collect(self, state, action, action_prob, extrinsic_reward, intrinsic_reward, done, i_episode):\n",
    "        if (self.trajectories[i_episode] == None):\n",
    "            self.trajectories[i_episode] = deque(maxlen=N_STEPS)\n",
    "        self.trajectories[i_episode].append((state, action, action_prob, extrinsic_reward, intrinsic_reward, done))\n",
    "        \n",
    "    def calculate_advantages(self, reward_standard_deviation_estimate):\n",
    "        self.advantages = []\n",
    "        self.extrinsic_TDerrors = []\n",
    "        self.intrinsic_TDerrors = [] #list of all the delta, used to uopdate the critic\n",
    "        \n",
    "        for trajectory in self.trajectories:\n",
    "            \n",
    "            advantage_trajectory = [] #list of advantages for each element in a single trajectory\n",
    "            e_delta = []\n",
    "            i_delta = []\n",
    "            \n",
    "            e_old_advantage = trajectory[-1][self.EXTRINSIC_REWARD]\n",
    "            e_delta.append(e_old_advantage)\n",
    "            \n",
    "            #normalizing the intrinisc reward before calculating the TDError\n",
    "            i_old_advantage = np.clip(trajectory[-1][self.INTRINSIC_REWARD] / reward_standard_deviation_estimate, a_min =-5, a_max = 5)\n",
    "            i_delta.append(i_old_advantage)\n",
    "            \n",
    "            advantage_trajectory.append(e_old_advantage + i_old_advantage)\n",
    "\n",
    "            for i in range(len(trajectory)-2,-1,-1):\n",
    "                e_delta.append(trajectory[i][self.EXTRINSIC_REWARD] + self.gamma*ppo.return_v_extrinsic_values(trajectory[i+1][self.STATE]) - ppo.return_v_extrinsic_values(trajectory[i][self.STATE]))\n",
    "                new_advantage = e_delta[-1] + self.gamma*e_old_advantage  \n",
    "                \n",
    "                e_old_advantage = new_advantage\n",
    "                \n",
    "                normalized_intrinsic_reward = np.clip(trajectory[-1][self.INTRINSIC_REWARD] / reward_standard_deviation_estimate, a_min =-5, a_max = 5)\n",
    "                i_delta.append(normalized_intrinsic_reward + self.gamma*ppo.return_v_intrinsic_values(trajectory[i+1][self.STATE]) - ppo.return_v_intrinsic_values(trajectory[i][self.STATE]))\n",
    "                new_advantage = i_delta[-1] + self.gamma*i_old_advantage \n",
    "                \n",
    "                i_old_advantage = new_advantage\n",
    "                \n",
    "                advantage_trajectory.append(i_old_advantage[0] + e_old_advantage[0])  \n",
    "               \n",
    "            #reverse the list (at pos 0 there is the last advantage/delta)\n",
    "            e_delta = list(reversed(e_delta))\n",
    "            i_delta = list(reversed(i_delta))\n",
    "        \n",
    "            self.extrinsic_TDerrors.append(e_delta)\n",
    "            self.intrinsic_TDerrors.append(i_delta)\n",
    "            \n",
    "            self.advantages.append(list(reversed(advantage_trajectory)))\n",
    "            \n",
    "        #flat all trajectories in a single deque adding the advantages (easier to sample random batches)\n",
    "        self.flat_trajectories(self.trajectories, self.advantages, self.extrinsic_TDerrors, self.intrinsic_TDerrors)\n",
    "    \n",
    "    def flat_trajectories(self, trajectories, advantages, extrinsic_TDerrors, intrinsic_TDerrors):\n",
    "        \n",
    "        size = 0\n",
    "        for trajectory in trajectories:\n",
    "            size = size + len(trajectory)\n",
    "        \n",
    "        self.flatten_trajectories = deque(maxlen=size)\n",
    "        \n",
    "        for trajectory, advantage, e_delta, i_delta in zip(trajectories, advantages, extrinsic_TDerrors, intrinsic_TDerrors):\n",
    "            for i in range(len(trajectory)):\n",
    "                self.flatten_trajectories.append((trajectory[i][self.STATE], trajectory[i][self.ACTION], trajectory[i][self.ACTION_PROB], trajectory[i][self.EXTRINSIC_REWARD], trajectory[i][self.INTRINSIC_REWARD], advantage[i], e_delta[i], i_delta[i], trajectory[i][self.DONE]))\n",
    "        \n",
    "        \n",
    "    #pick a random batch example from the flatten list of trajectories\n",
    "    def sample_experiences(self, batch_size):\n",
    "        if (len(self.flatten_trajectories) >= batch_size):\n",
    "            indices = np.random.permutation(len(self.flatten_trajectories))[:batch_size]\n",
    "        else:\n",
    "            indices = np.random.permutation(len(self.flatten_trajectories))\n",
    "        batch = [self.flatten_trajectories[index] for index in indices]\n",
    "        #delete form the memory the used obervations\n",
    "        for index in sorted(indices, reverse=True):\n",
    "            del self.flatten_trajectories[index]\n",
    "        states, actions, actions_prob, e_rewards, i_rewards, advantages, e_TDerrors, i_TDerrors, dones = [np.array([experience[field_index] for experience in batch]) for field_index in range(9)]\n",
    "        return states, actions, actions_prob, e_rewards, i_rewards, advantages, e_TDerrors, i_TDerrors, dones\n",
    "        \n",
    "    def reset(self):\n",
    "        for trajectory in self.trajectories:\n",
    "            trajectory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b87e1d",
   "metadata": {},
   "source": [
    "# RND class\n",
    "\n",
    "$ s_{t+1} $ is the observed state after the current action $ a_t $ \n",
    "\n",
    "Predictor update formula:\n",
    "$ \\theta_{t+1} = \\theta_t + \\alpha\\nabla min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc8a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RND(object):\n",
    "    \n",
    "    input_shape = [84,84,4] \n",
    "    n_outputs = 10\n",
    "    \n",
    "    N_intrinsic_rewards = 0 #number of intrinsic reward received\n",
    "    intrinisc_reward_mean = 0.0 #mean of the intrinsic rewards received\n",
    "    M2 = 0.0 #sum of squares of differences from the current mean\n",
    "    \n",
    "    def __init__(self, env, n_normalization_steps):\n",
    "        self.target = self.create_target()\n",
    "        self.predictor = self.create_predictor()\n",
    "        \n",
    "        self.MSE = tf.keras.losses.mean_squared_error\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        self.initialize_standard_deviation_estimate(env, n_normalization_steps)\n",
    "        \n",
    "    #create the NN of the target\n",
    "    def create_target(self):\n",
    "        target = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(self.n_outputs) ])\n",
    "        return target\n",
    "        \n",
    "    #create the NN of the predictor\n",
    "    def create_predictor(self):\n",
    "        predictor = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(self.n_outputs) ])\n",
    "        return predictor\n",
    "    \n",
    "    def train_predictor(self, observations):\n",
    "        # extrinsic critic (rewards from the envirnoment)\n",
    "        target_values = self.target.predict(observations)#tf.reshape(tf.convert_to_tensor(np.asarray(self.alpha*extrinsic_TDerrors).astype('float32')), (len(states), 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_values = self.predictor(observations)\n",
    "            #v_values = tf.reduce_sum(all_values, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(self.MSE(target_values, all_values))\n",
    "        grads = tape.gradient(loss, self.predictor.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.predictor.trainable_variables))\n",
    "        \n",
    "    def calculate_intrinsic_reward(self, observation):\n",
    "        f_target = self.target.predict(tf.expand_dims(observation, axis=0))\n",
    "        f_predictor = self.predictor.predict(tf.expand_dims(observation, axis=0))\n",
    "        return self.MSE(f_target, f_predictor)\n",
    "    \n",
    "    def initialize_standard_deviation_estimate(self, env, n_normalization_steps):\n",
    "        obsevation = env.reset()\n",
    "        \n",
    "        for i_step in range(n_normalization_steps):\n",
    "            random_action = env.action_space.sample()\n",
    "            new_obs, reward, done, info = env.step(random_action)\n",
    "            \n",
    "            obs1, reward, done, info = env.step(0)\n",
    "            obs2, reward, done, info = env.step(0)\n",
    "            obs3, reward, done, info = env.step(0)\n",
    "\n",
    "            observation = tf.transpose(tf.squeeze(tf.stack([new_obs, obs1, obs2, obs3], axis = 0)), [1,2,0])\n",
    "            \n",
    "            self.update_observation_normalization_param(observation)\n",
    "    \n",
    "    def update_observation_normalization_param(self, observation):\n",
    "        return\n",
    "    \n",
    "    #Using welford's algorithm\n",
    "    def update_reward_normalization_param(self, i_reward):\n",
    "        self.N_intrinsic_rewards = self.N_intrinsic_rewards + 1\n",
    "        delta = i_reward - self.intrinisc_reward_mean\n",
    "        intrinisc_reward_mean = self.intrinisc_reward_mean + delta/self.N_intrinsic_rewards # mean_N = mean_{N-1} + (i_t - mean_{N-1}) / N\n",
    "        self.M2 = self.M2 + delta*(i_reward - self.intrinisc_reward_mean)\n",
    "        \n",
    "    def calculate_reward_standard_deviation(self):\n",
    "        standard_deviation = math.sqrt( self.M2 / (self.N_intrinsic_rewards - 1))\n",
    "        print(\"===============================================================\")\n",
    "        print(\"===============================================================\")\n",
    "        print(\"STANDARD DEVIATION {}\".format(standard_deviation))\n",
    "        print(\"===============================================================\")\n",
    "        print(\"===============================================================\")\n",
    "        return standard_deviation\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae005be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4252926563.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [4]\u001b[1;36m\u001b[0m\n\u001b[1;33m    env.close()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "basic_env = gym.make(\"GravitarNoFrameskip-v4\", obs_type = \"image\")\n",
    "env = AtariPreprocessing(basic_env)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "random_action = env.action_space.sample()\n",
    "new_obs, reward, done, info = env.step(random_action)\n",
    "            \n",
    "obs1, reward, done, info = env.step(0)\n",
    "obs2, reward, done, info = env.step(0)\n",
    "obs3, reward, done, info = env.step(0)\n",
    "\n",
    "observation = tf.transpose(tf.squeeze(tf.stack([new_obs, obs1, obs2, obs3], axis = 0)), [1,2,0])\n",
    "\n",
    "for obs_dim in np.moveaxis(observation, -1, 0):\n",
    "    \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875cd80",
   "metadata": {},
   "source": [
    "# PPO class\n",
    "\n",
    "$ s_{t+1} $ is the observed state after the current action $ a_t $ \n",
    "\n",
    "Actor update formula:\n",
    "$ \\theta_{t+1} = \\theta_t + \\alpha\\nabla min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t)$\n",
    "\n",
    "Critic update formula:\n",
    "$ w_{t+1} = w_t + \\alpha\\delta_t\\nabla\\hat{v}(s_t,w)$\n",
    "\n",
    "Probability ratio $ r_t(\\theta) \\doteq $\n",
    "$ \\pi_\\theta(a_t | s_t) \\over \\pi_{\\theta_{old}}(a_t | s_t) $\n",
    "\n",
    "Advantage:\n",
    "$ \\hat{A}_t \\doteq \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + (\\gamma\\lambda)^2\\delta_{t+2} + ... + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1} = \\delta_t + (\\gamma\\lambda)\\hat{A}_{t+1}$\n",
    "\n",
    "TDerror:\n",
    "$ \\quad \\delta_t  \\doteq $\n",
    "$ r_t + \\gamma\\hat{v}(s_{t+1},w) - \\hat{v}(s_t,w) $ $ \\qquad $ (if $ s_{t+1} $ is terminal then $ \\hat{v}(s_{t+1},w) = 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62b137fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(object):\n",
    "    \n",
    "    input_shape = [84,84,4] \n",
    "    n_outputs = 6 #wrapped_env.action_space.n\n",
    "    \n",
    "    def __init__(self, env, n_episodes = 1, train_steps = 100, epsilon = 0.2, alpha = 0.95, n_normalization_steps = 300):\n",
    "        self.actor = self.create_actor()\n",
    "        self.intrinsic_critic = self.create_critic()\n",
    "        self.extrinsic_critic = self.create_critic()\n",
    "        \n",
    "        self.MSE = tf.keras.losses.mean_squared_error\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        #self.n_outputs = env.action_space.n\n",
    "        \n",
    "        self.train_steps = train_steps\n",
    "        \n",
    "        self.memory = Memory(n_episodes)\n",
    "        \n",
    "        self.rnd = RND(env, n_normalization_steps)\n",
    "        \n",
    "    #create the NN of the actor\n",
    "    # Given the state returns the probability of each action\n",
    "    def create_actor(self):    \n",
    "        actor = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(self.n_outputs, activation = 'softmax') ])\n",
    "        return actor\n",
    "       \n",
    "    #create the NN of the critic\n",
    "    # Given the state returns the value function\n",
    "    def create_critic(self):\n",
    "        critic = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(1) ])\n",
    "        \n",
    "        return critic\n",
    "      \n",
    "    def play_one_step(self, env, observation, i_episode):\n",
    "        action, action_prob = self.select_action(observation)\n",
    "        observation, e_reward, done, info = env.step(action)\n",
    "        \n",
    "        i_reward = 0\n",
    "        \n",
    "        #put in wrapper\n",
    "        e_reward = float(e_reward)/100.\n",
    "        \n",
    "        obs1, r, d, i = env.step(0)\n",
    "        obs2, r, d, i = env.step(0)\n",
    "        obs3, r, d, i = env.step(0)\n",
    "\n",
    "        observation = tf.transpose(tf.squeeze(tf.stack([observation, obs1, obs2, obs3], axis = 0)), [1,2,0])\n",
    "        \n",
    "        i_reward = self.rnd.calculate_intrinsic_reward(observation)\n",
    "        \n",
    "        self.rnd.update_reward_normalization_param(i_reward)\n",
    "        \n",
    "        self.memory.collect(observation, action, action_prob, e_reward, i_reward, done, i_episode)\n",
    "        \n",
    "        return observation, action, e_reward, i_reward, done, info\n",
    "        \n",
    "    #select the action (returned as a number)\n",
    "    def select_action(self, observation):\n",
    "        \n",
    "        # explanation: tf.expand_dims(observation['pov'], axis=0)\n",
    "        # since we pass another input of shape (1,) -> we need to tell keras that is one image (it assumes the first dimension to be the batch)\n",
    "        action_probabilities = self.actor.predict(tf.expand_dims(observation, axis=0))[0]\n",
    "        \n",
    "        #choosing an action usign randomly using a \"roulette wheel\" approach\n",
    "        r = random.random()\n",
    "        \n",
    "        sum_probabilities = 0\n",
    "        for i in range(len(action_probabilities)):\n",
    "            sum_probabilities = sum_probabilities + action_probabilities[i]\n",
    "            \n",
    "            if (r <= sum_probabilities):\n",
    "                action = i\n",
    "                break\n",
    "        \n",
    "        return action, action_probabilities[action]\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        self.memory.calculate_advantages(self.rnd.calculate_reward_standard_deviation())\n",
    "        \n",
    "        for i_step in range(self.train_steps):\n",
    "            done = self.training_step(batch_size)\n",
    "            if (done):\n",
    "                break\n",
    "        \n",
    "        self.memory.reset()\n",
    "        \n",
    "    #training done on the memory (the advantages must be calculated before hand)\n",
    "    def training_step(self, batch_size):\n",
    "        #get experiences (parts of a trajectory) from the memory\n",
    "        experiences = self.memory.sample_experiences(batch_size)\n",
    "        \n",
    "        states, actions, actions_prob, extrinsic_rewards, intrinsic_rewards, advantages, extrinsic_TDerrors, intrinsic_TDerrors, dones = experiences\n",
    "        \n",
    "        done = False\n",
    "        if (len(states) != batch_size):\n",
    "            done = True\n",
    "        \n",
    "        #compute the values for the update of the actor\n",
    "        \n",
    "        mask = tf.one_hot(actions, self.n_outputs)\n",
    "        \n",
    "        states = states/255\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            current_actions_prob = self.actor(states)\n",
    "            \n",
    "            current_action_prob = tf.reduce_sum(current_actions_prob*mask, axis=1, keepdims=True)\n",
    "            old_actions_prob = tf.reshape(tf.convert_to_tensor(actions_prob), [len(states), 1])\n",
    "            probability_ratio = tf.divide(current_action_prob, old_actions_prob)\n",
    "        \n",
    "            surrogate_arg_1 = tf.convert_to_tensor([probability_ratio[index]*advantages[index] for index in range(len(advantages))])\n",
    "            surrogate_arg_2 = tf.convert_to_tensor(np.array([tf.keras.backend.clip(probability_ratio,1-self.epsilon,1+self.epsilon)[index]*advantages[index] for index in range(len(advantages))]).flatten())\n",
    "            \n",
    "            L = 0 - tf.minimum( surrogate_arg_1 , surrogate_arg_2 ) \n",
    "            loss = tf.reduce_mean(L)\n",
    "\n",
    "        actor_weights = self.actor.trainable_variables\n",
    "        grads = tape.gradient(loss, actor_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, actor_weights))\n",
    "        \n",
    "        #update of the critic. The target is the TD error\n",
    "        \n",
    "        # extrinsic critic (rewards from the envirnoment)\n",
    "        target_v_values = tf.reshape(tf.convert_to_tensor(np.asarray(self.alpha*extrinsic_TDerrors).astype('float32')), (len(states), 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_v_values = self.extrinsic_critic(states)\n",
    "            v_values = tf.reduce_sum(all_v_values*mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(self.MSE(target_v_values, v_values))\n",
    "        grads = tape.gradient(loss, self.extrinsic_critic.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.extrinsic_critic.trainable_variables))\n",
    "        \n",
    "        # intrinsic critic (rewards from the exploration)\n",
    "        target_v_values = tf.reshape(tf.convert_to_tensor(np.asarray(self.alpha*intrinsic_TDerrors).astype('float32')), (len(states), 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_v_values = self.intrinsic_critic(states)\n",
    "            v_values = tf.reduce_sum(all_v_values*mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(self.MSE(target_v_values, v_values))\n",
    "        grads = tape.gradient(loss, self.intrinsic_critic.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.intrinsic_critic.trainable_variables))\n",
    "        \n",
    "        #since v changed we need to re-calculate the advantages\n",
    "        #self.memory.calculate_advantages()\n",
    "        \n",
    "        self.rnd.train_predictor(states)\n",
    "        \n",
    "        return done\n",
    "    \n",
    "    def return_v_extrinsic_values(self, observation):\n",
    "        v_e = self.extrinsic_critic.predict(tf.expand_dims(observation, axis=0))[0]\n",
    "        return v_e\n",
    "    \n",
    "    def return_v_intrinsic_values(self, observation):\n",
    "        v_i = self.intrinsic_critic.predict(tf.expand_dims(observation, axis=0))[0]\n",
    "        return v_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2869d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45a4f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class collect_trajectory(Thread):\n",
    "    \n",
    "    def __init__(self, i_agent = 1):\n",
    "         \n",
    "        Thread.__init__(self)   \n",
    "        self.n_agent = i_agent\n",
    "        self.rewards = [] \n",
    "        \n",
    "        basic_env = gym.make(\"GravitarNoFrameskip-v4\", obs_type = \"image\")\n",
    "        self.wrapped_env = AtariPreprocessing(basic_env)\n",
    "            \n",
    "    def run(self):\n",
    "        print(\"Starting {}\".format(self.n_agent))\n",
    "        \n",
    "        observation = self.wrapped_env.reset()\n",
    "        \n",
    "        obs1, reward, done, info = self.wrapped_env.step(0)\n",
    "        obs2, reward, done, info = self.wrapped_env.step(0)\n",
    "        obs3, reward, done, info = self.wrapped_env.step(0)\n",
    "\n",
    "        observation = tf.transpose(tf.squeeze(tf.stack([observation, obs1, obs2, obs3], axis = 0)), [1,2,0])\n",
    "        \n",
    "        extrinsic_episode_reward = 0.0\n",
    "        intrinsic_episode_reward = 0.0\n",
    "        self.n_episodes = 0.0\n",
    "        self.extrinsic_tot_reward = 0.0\n",
    "        self.intrinsic_tot_reward = 0.0\n",
    "        \n",
    "        for i_step in range(N_STEPS):   \n",
    "            \n",
    "            observation, action, extrinsic_reward, intrinsic_reward, done, info = ppo.play_one_step(self.wrapped_env, observation, self.n_agent)\n",
    "\n",
    "            #wrapped_env.render()\n",
    "            extrinsic_episode_reward = extrinsic_episode_reward + extrinsic_reward\n",
    "            intrinsic_episode_reward = intrinsic_episode_reward + intrinsic_reward\n",
    "            \n",
    "            #continuing task. if an episode is done we continue until complting the number of steps\n",
    "            if (done):\n",
    "                observation = self.wrapped_env.reset()\n",
    "        \n",
    "                obs1, reward, done, info = self.wrapped_env.step(0)\n",
    "                obs2, reward, done, info = self.wrapped_env.step(0)\n",
    "                obs3, reward, done, info = self.wrapped_env.step(0)\n",
    "\n",
    "                observation = tf.transpose(tf.squeeze(tf.stack([observation, obs1, obs2, obs3], axis = 0)), [1,2,0])\n",
    "                \n",
    "                self.extrinsic_tot_reward = self.extrinsic_tot_reward + extrinsic_episode_reward\n",
    "                self.intrinsic_tot_reward = self.intrinsic_tot_reward + intrinsic_episode_reward\n",
    "                self.n_episodes = self.n_episodes + 1\n",
    "                episode_reward = 0.0\n",
    "        \n",
    "        self.wrapped_env.close()\n",
    "        print(\"Exiting {} average ex reward: {} in reward: {}\".format(self.n_agent, self.extrinsic_tot_reward/self.n_episodes, self.intrinsic_tot_reward/self.n_episodes))\n",
    "        \n",
    "    def get_reward_average(self):\n",
    "        if (self.n_episodes > 0):\n",
    "            return (self.extrinsic_tot_reward/self.n_episodes, self.intrinsic_tot_reward/self.n_episodes)\n",
    "        else:\n",
    "            return (self.extrinsic_tot_reward, self.intrinsic_tot_reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700dfc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 0\n",
      "Starting 1\n",
      "Starting 2\n",
      "Starting 3\n",
      "Starting 4\n",
      "Exiting 2 average ex reward: 0.0 in reward: [963.3509]\n",
      "Exiting 3 average ex reward: 0.0 in reward: [1020.39844]\n",
      "Exiting 4 average ex reward: 0.0 in reward: [1169.9598]\n",
      "Exiting 0 average ex reward: 0.0 in reward: [1067.2313]\n",
      "Exiting 1 average ex reward: 0.0 in reward: [1028.1621]\n",
      "Epoch: 0 ended with average extrinsic reward: 0.0 intrinsic reward [1049.8206] \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 2.3359872053592876\n",
      "===============================================================\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "N_STEPS_NORMALIZATION = 500\n",
    "N_EPOCHS = 100\n",
    "N_EPISODES = 5 # in multi-agent this is the number of agents (each agnet collect 1 trajectory)\n",
    "N_STEPS = 900 # max number of step for each episode\n",
    "\n",
    "TRAIN_STEPS = 30 # number of max steps done during training. if the number of samples is less than TRAIN_STEPS*BATCH_SIZE will stop early after completing the training on all the samples\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#env used to initialize the parameters inside PPO and RND\n",
    "env = gym.make(\"GravitarNoFrameskip-v4\", obs_type = \"image\")\n",
    "wrapped_env = AtariPreprocessing(env)\n",
    "\n",
    "ppo = PPO(wrapped_env, n_episodes = N_EPISODES, train_steps = TRAIN_STEPS, n_normalization_steps = N_STEPS_NORMALIZATION)\n",
    "\n",
    "e_rewards = []\n",
    "i_rewards = []\n",
    "\n",
    "for i_epoch in range(N_EPOCHS):\n",
    "    extrinsic_epoch_reward = 0.0\n",
    "    intrinsic_epoch_reward = 0.0\n",
    "    agents = []\n",
    "    for i_agent in range(N_EPISODES):\n",
    "        agents.append(collect_trajectory(i_agent = i_agent))\n",
    "    for agent in agents:\n",
    "        agent.start()\n",
    "    for agent in agents:\n",
    "        agent.join()\n",
    "        extrinsic_reward_average, intrinsic_reward_average = agent.get_reward_average()\n",
    "        extrinsic_epoch_reward = extrinsic_epoch_reward + extrinsic_reward_average\n",
    "        intrinsic_epoch_reward = intrinsic_epoch_reward + intrinsic_reward_average\n",
    "    e_rewards.append(extrinsic_epoch_reward/N_EPISODES)\n",
    "    i_rewards.append(intrinsic_epoch_reward/N_EPISODES)\n",
    "    print(\"Epoch: {} ended with average extrinsic reward: {} intrinsic reward {} \\n\".format(i_epoch, extrinsic_epoch_reward/N_EPISODES, intrinsic_epoch_reward/N_EPISODES) )  \n",
    "    ppo.train(batch_size = 32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea6744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wrappend_env.reset()\n",
    "#for i_step in range(N_STEPS_NORMALIZATION):\n",
    "    \n",
    "#    action = wrappend_env.action_space.sample() \n",
    "#    observation, reward, done, info = wrappend_env.step(action)\n",
    "    #update observation normalization parameters usig s_t+1\n",
    "#    t = t +1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5b122",
   "metadata": {},
   "source": [
    "## Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(205)#N_EPOCHS)\n",
    "\n",
    "plt.plot(epochs, i_rewards)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d710415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
