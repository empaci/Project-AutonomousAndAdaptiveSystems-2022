{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b284cff8",
   "metadata": {},
   "source": [
    "# AirRaid-PPO implementation\n",
    "\n",
    "PPO test in the AirRaid environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7d37ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing\n",
    "from gym.wrappers import FrameStack\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "from collections import deque\n",
    "import random\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04544b5e",
   "metadata": {},
   "source": [
    "## Preparing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5dca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_env = gym.make(\"GravitarNoFrameskip-v4\", obs_type = \"image\")\n",
    "wrapped_env = AtariPreprocessing(basic_env)\n",
    "stack_env = FrameStack(wrapped_env, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c963de",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Class used to memorize the trajectory and calculate the advntage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23b7a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    \n",
    "    STATE = 0\n",
    "    ACTION = 1\n",
    "    ACTION_PROB = 2\n",
    "    REWARD = 3\n",
    "    DONE = 4\n",
    "    \n",
    "    def __init__(self, n_trajectories, gamma = 0.4):\n",
    "        self.trajectories = np.empty(n_trajectories, dtype=object)\n",
    "        self.gamma = gamma\n",
    "              \n",
    "    def collect(self, state, action, action_prob, reward, done, i_episode):\n",
    "        if (self.trajectories[i_episode] == None):\n",
    "            self.trajectories[i_episode] = deque(maxlen=N_STEPS)\n",
    "        self.trajectories[i_episode].append((state, action, action_prob, reward, done))\n",
    "        \n",
    "    def calculate_advantages(self):\n",
    "        self.advantages = []\n",
    "        self.TDerrors = [] #list of all the delta, used to uopdate the critic\n",
    "        \n",
    "        for trajectory in self.trajectories:\n",
    "            \n",
    "            advantage_trajectory = [] #list of advantages for each element in a single trajectory\n",
    "            delta = []\n",
    "            \n",
    "            #print(trajectory[-1])\n",
    "            \n",
    "            old_advantage = trajectory[-1][self.REWARD]\n",
    "            delta.append(old_advantage)\n",
    "            advantage_trajectory.append(old_advantage)\n",
    "\n",
    "            for i in range(len(trajectory)-2,-1,-1):\n",
    "                delta.append(trajectory[i][self.REWARD] + self.gamma*ppo.return_v_values(trajectory[i+1][self.STATE]) - ppo.return_v_values(trajectory[i][self.STATE]))\n",
    "                new_advantage = delta[-1] + self.gamma*old_advantage\n",
    "                \n",
    "                advantage_trajectory.append(new_advantage[0])   \n",
    "                \n",
    "                old_advantage = new_advantage\n",
    "               \n",
    "            #reverse the list (at pos 0 there is the last advantage/delta)\n",
    "            advantage_trajectory = list(reversed(advantage_trajectory))\n",
    "            delta = list(reversed(delta))\n",
    "            \n",
    "            self.advantages.append(advantage_trajectory)\n",
    "            self.TDerrors.append(delta)\n",
    "            \n",
    "        #flat all trajectories in a single deque adding the advantages (easier to sample random batches)\n",
    "        self.flat_trajectories(self.trajectories, self.advantages, self.TDerrors)\n",
    "    \n",
    "    def flat_trajectories(self, trajectories, advantages, TDerrors):\n",
    "        \n",
    "        size = 0\n",
    "        for trajectory in trajectories:\n",
    "            size = size + len(trajectory)\n",
    "        \n",
    "        self.flatten_trajectories = deque(maxlen=size)\n",
    "        \n",
    "        for trajectory, advantage, delta in zip(trajectories, advantages, TDerrors):\n",
    "            for i in range(len(trajectory)):\n",
    "                self.flatten_trajectories.append((trajectory[i][self.STATE], trajectory[i][self.ACTION], trajectory[i][self.ACTION_PROB], trajectory[i][self.REWARD], advantage[i], delta[i], trajectory[i][self.DONE]))\n",
    "        \n",
    "        \n",
    "    #pick a random batch example from the flatten list of trajectories\n",
    "    def sample_experiences(self, batch_size):\n",
    "        if (len(self.flatten_trajectories) >= batch_size):\n",
    "            indices = np.random.permutation(len(self.flatten_trajectories))[:batch_size]\n",
    "        else:\n",
    "            indices = np.random.permutation(len(self.flatten_trajectories))\n",
    "        batch = [self.flatten_trajectories[index] for index in indices]\n",
    "        #delete form the memory the used obervations\n",
    "        for index in sorted(indices, reverse=True):\n",
    "            del self.flatten_trajectories[index]\n",
    "        states, actions, actions_prob, rewards, advantages, TDerrors, dones = [np.array([experience[field_index] for experience in batch]) for field_index in range(7)]\n",
    "        return states, actions, actions_prob, rewards, advantages, TDerrors, dones\n",
    "        \n",
    "    def reset(self):\n",
    "        for trajectory in self.trajectories:\n",
    "            trajectory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875cd80",
   "metadata": {},
   "source": [
    "# PPO class\n",
    "\n",
    "$ s_{t+1} $ is the observed state after the current action $ a_t $ \n",
    "\n",
    "Actor update formula:\n",
    "$ \\theta_{t+1} = \\theta_t + \\alpha\\nabla min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t)$\n",
    "\n",
    "Critic update formula:\n",
    "$ w_{t+1} = w_t + \\alpha\\delta_t\\nabla\\hat{v}(s_t,w)$\n",
    "\n",
    "Probability ratio $ r_t(\\theta) \\doteq $\n",
    "$ \\pi_\\theta(a_t | s_t) \\over \\pi_{\\theta_old}(a_t | s_t) $\n",
    "\n",
    "Advantage:\n",
    "$ \\hat{A}_t \\doteq \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + (\\gamma\\lambda)^2\\delta_{t+2} + ... + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1} = \\delta_t + (\\gamma\\lambda)\\hat{A}_{t+1}$\n",
    "\n",
    "TDerror:\n",
    "$ \\quad \\delta_t  \\doteq $\n",
    "$ r_t + \\gamma\\hat{v}(s_{t+1},w) - \\hat{v}(s_t,w) $ $ \\qquad $ (if $ s_{t+1} $ is terminal then $ \\hat{v}(s_{t+1},w) = 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62b137fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(object):\n",
    "    \n",
    "    input_shape = [4,84,84] \n",
    "    n_outputs = stack_env.action_space.n\n",
    "    \n",
    "    def __init__(self, n_episodes = 1, train_steps = 100, epsilon = 0.2, alpha = 0.95):\n",
    "        self.actor = self.create_actor()\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        self.critic = self.create_critic()\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.train_steps = train_steps\n",
    "        \n",
    "        self.memory = Memory(n_episodes)\n",
    "        \n",
    "    #create the NN of the actor\n",
    "    # Given the state returns the probability of each action\n",
    "    def create_actor(self):    \n",
    "        actor = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(self.n_outputs, activation = 'softmax') ])\n",
    "        return actor\n",
    "       \n",
    "    #create the NN of the critic\n",
    "    # Given the state returns the value function\n",
    "    def create_critic(self):\n",
    "        critic = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(1) ])\n",
    "        \n",
    "        self.critic_loss_fn = tf.keras.losses.mean_squared_error\n",
    "        \n",
    "        return critic\n",
    "      \n",
    "    def play_one_step(self, env, observation):\n",
    "        action, action_prob = self.select_action(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        #put in wrapper\n",
    "        reward = float(reward)/100.\n",
    "        \n",
    "        self.memory.collect(observation, action, action_prob, reward, done, i_episode)\n",
    "        return observation, action, reward, done, info\n",
    "        \n",
    "    #select the action (returned as a number)\n",
    "    def select_action(self, observation):\n",
    "        \n",
    "        # explanation: tf.expand_dims(observation['pov'], axis=0)\n",
    "        # since we pass another input of shape (1,) -> we need to tell keras that is one image (it assumes the first dimension to be the batch)\n",
    "        action_probabilities = self.actor.predict(tf.expand_dims(observation, axis=0))[0]\n",
    "        \n",
    "        #choosing an action usign randomly using a \"roulette wheel\" approach\n",
    "        r = random.random()\n",
    "        \n",
    "        sum_probabilities = 0\n",
    "        for i in range(len(action_probabilities)):\n",
    "            sum_probabilities = sum_probabilities + action_probabilities[i]\n",
    "            \n",
    "            if (r <= sum_probabilities):\n",
    "                action = i\n",
    "                break\n",
    "        \n",
    "        return action, action_probabilities[action]\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        self.memory.calculate_advantages()\n",
    "        \n",
    "        for i_step in range(self.train_steps):\n",
    "            done = self.training_step(batch_size)\n",
    "            if (done):\n",
    "                break\n",
    "        \n",
    "        self.memory.reset()\n",
    "        \n",
    "    #training done on the memory (the advantages must be calculated before hand)\n",
    "    def training_step(self, batch_size):\n",
    "        #get experiences (parts of a trajectory) from the memory\n",
    "        experiences = self.memory.sample_experiences(batch_size)\n",
    "        states, actions, actions_prob, rewards, advantages, TDerrors, dones = experiences\n",
    "        \n",
    "        done = False\n",
    "        if (len(states) != batch_size):\n",
    "            done = True\n",
    "        \n",
    "        #compute the values for the update of the actor\n",
    "        \n",
    "        mask = tf.one_hot(actions, self.n_outputs)\n",
    "\n",
    "        states = np.array(states)\n",
    "        states = states/255\n",
    "\n",
    "        #array of shape (64,) into array of shape (64,1)\n",
    "        #states =  np.array(np.array_split(states, len(states)))\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            current_actions_prob = self.actor(states)\n",
    "            \n",
    "            current_action_prob = tf.reduce_sum(current_actions_prob*mask, axis=1, keepdims=True)\n",
    "            old_actions_prob = tf.reshape(tf.convert_to_tensor(actions_prob), [len(states), 1])\n",
    "            probability_ratio = tf.divide(current_action_prob, old_actions_prob)\n",
    "        \n",
    "            surrogate_arg_1 = tf.convert_to_tensor([probability_ratio[index]*advantages[index] for index in range(len(advantages))])\n",
    "            surrogate_arg_2 = tf.convert_to_tensor(np.array([tf.keras.backend.clip(probability_ratio,1-self.epsilon,1+self.epsilon)[index]*advantages[index] for index in range(len(advantages))]).flatten())\n",
    "            \n",
    "            L = 0 - tf.minimum( surrogate_arg_1 , surrogate_arg_2 ) \n",
    "            loss = tf.reduce_mean(L)\n",
    "\n",
    "        actor_weights = self.actor.trainable_variables\n",
    "        grads = tape.gradient(loss, actor_weights)\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, actor_weights))\n",
    "        \n",
    "        #update of the critic. We need the target is the TD error\n",
    "        target_v_values = tf.reshape(tf.convert_to_tensor(np.asarray(self.alpha*TDerrors).astype('float32')), (len(states), 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_v_values = self.critic(states)\n",
    "            v_values = tf.reduce_sum(all_v_values*mask, axis=1, keepdims=True)\n",
    "            loss = tf.reduce_mean(self.critic_loss_fn(target_v_values, v_values))\n",
    "        grads = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))\n",
    "        \n",
    "        #since v changed we need to re-calculate the advantages\n",
    "        #self.memory.calculate_advantages()\n",
    "        \n",
    "        return done\n",
    "    \n",
    "    def return_v_values(self, observation):\n",
    "        v = self.critic.predict(tf.expand_dims(observation, axis=0))[0]\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2869d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bea6744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Episode 0 terminated after 405 steps with total reward: 350.0\n",
      "Epoch 0 Episode 1 terminated after 336 steps with total reward: 500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paci3\\AppData\\Local\\Temp\\ipykernel_23520\\969158661.py:74: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  states, actions, actions_prob, rewards, advantages, TDerrors, dones = [np.array([experience[field_index] for experience in batch]) for field_index in range(7)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Episode 0 terminated after 514 steps with total reward: 0.0\n",
      "Epoch 1 Episode 1 terminated after 506 steps with total reward: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(epoch_reward\u001b[38;5;241m/\u001b[39mN_EPISODES)\n\u001b[1;32m---> 37\u001b[0m     \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m stack_env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size):\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_advantages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_steps):\n\u001b[0;32m     81\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(batch_size)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mMemory.calculate_advantages\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m advantage_trajectory\u001b[38;5;241m.\u001b[39mappend(old_advantage)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(trajectory)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 34\u001b[0m     delta\u001b[38;5;241m.\u001b[39mappend(trajectory[i][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREWARD] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39mppo\u001b[38;5;241m.\u001b[39mreturn_v_values(trajectory[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATE]) \u001b[38;5;241m-\u001b[39m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_v_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectory\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTATE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     35\u001b[0m     new_advantage \u001b[38;5;241m=\u001b[39m delta[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39mold_advantage\n\u001b[0;32m     37\u001b[0m     advantage_trajectory\u001b[38;5;241m.\u001b[39mappend(new_advantage[\u001b[38;5;241m0\u001b[39m])   \n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mPPO.return_v_values\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreturn_v_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[1;32m--> 140\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py:1747\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1745\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_begin()\n\u001b[0;32m   1746\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1747\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\keras\\engine\\data_adapter.py:1180\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1180\u001b[0m   data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1181\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:411\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    410\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__iter__() is only supported inside of tf.function \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    414\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    695\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_message)\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:719\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(ds_variant):\n\u001b[0;32m    715\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deleter \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    716\u001b[0m       gen_dataset_ops\u001b[38;5;241m.\u001b[39manonymous_iterator_v2(\n\u001b[0;32m    717\u001b[0m           output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[0;32m    718\u001b[0m           output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes))\n\u001b[1;32m--> 719\u001b[0m   \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    720\u001b[0m   \u001b[38;5;66;03m# Delete the resource when this object is deleted\u001b[39;00m\n\u001b[0;32m    721\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resource_deleter \u001b[38;5;241m=\u001b[39m IteratorResourceDeleter(\n\u001b[0;32m    722\u001b[0m       handle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource,\n\u001b[0;32m    723\u001b[0m       deleter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deleter)\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3119\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3118\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3119\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3120\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3122\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "N_EPISODES = 2 # number of trajectories collected in one epoch\n",
    "N_STEPS = 900 # max number of step for each episode\n",
    "\n",
    "TRAIN_STEPS = 30 # number of max steps done during training. if the number of samples is less than TRAIN_STEPS*BATCH_SIZE will stop early after completing the training on all the samples\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "ppo = PPO(n_episodes = N_EPISODES, train_steps = TRAIN_STEPS)\n",
    "\n",
    "rewards = []\n",
    "\n",
    "\n",
    "for i_epoch in range(N_EPOCHS):\n",
    "    \n",
    "    epoch_reward = 0.0\n",
    "    \n",
    "    for i_episode in range(N_EPISODES):\n",
    "        \n",
    "        observation = stack_env.reset()\n",
    "        \n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        for i_step in range(N_STEPS):   \n",
    "            \n",
    "            observation, action, reward, done, info = ppo.play_one_step(stack_env, observation)\n",
    "\n",
    "            #stack_env.render()\n",
    "            episode_reward = episode_reward + reward\n",
    "\n",
    "            if(done or i_step == N_STEPS-1):\n",
    "                print(\"Epoch {} Episode {} terminated after {} steps with total reward: {}\".format(i_epoch, i_episode, i_step, episode_reward*100))\n",
    "                epoch_reward = epoch_reward + episode_reward\n",
    "                break\n",
    "                \n",
    "    rewards.append(epoch_reward/N_EPISODES)\n",
    "    \n",
    "    ppo.train(batch_size = 64)\n",
    "    \n",
    "stack_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5b122",
   "metadata": {},
   "source": [
    "## Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(N_EPOCHS)\n",
    "\n",
    "plt.plot(epochs, rewards)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d710415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
