{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b284cff8",
   "metadata": {},
   "source": [
    "# Atari-PPO-RND implementation\n",
    "\n",
    "PPO-RND test in the Atari environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d37ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing\n",
    "from gym.wrappers import FrameStack\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from collections import deque\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from threading import Thread\n",
    "import math\n",
    "\n",
    "# change keras setting to use the conv2d NN passing the channel first (as returned from the FrameStack wrapper)\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c963de",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Class used to memorize the trajectory and calculate the advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b7a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    \n",
    "    STATE = 0\n",
    "    ACTION = 1\n",
    "    ACTION_PROB = 2\n",
    "    EXTRINSIC_REWARD = 3\n",
    "    INTRINSIC_REWARD = 4\n",
    "    DONE = 5\n",
    "    \n",
    "    def __init__(self, n_trajectories, gamma = 0.98, e_lambda_p = 0.96, i_lambda_p = 0.99):\n",
    "        self.trajectories = np.empty(n_trajectories, dtype=object)\n",
    "        self.gamma = gamma\n",
    "        self.e_lambda_p = e_lambda_p\n",
    "        self.i_lambda_p = i_lambda_p\n",
    "              \n",
    "    def collect(self, state, action, action_prob, extrinsic_reward, intrinsic_reward, done, i_episode):\n",
    "        if (self.trajectories[i_episode] == None):\n",
    "            self.trajectories[i_episode] = deque(maxlen=N_STEPS)\n",
    "        self.trajectories[i_episode].append((state, action, action_prob, extrinsic_reward, intrinsic_reward, done))\n",
    "        \n",
    "    def calculate_advantages(self, reward_standard_deviation_estimate):\n",
    "        advantages = []\n",
    "        extrinsic_TDerrors = []\n",
    "        intrinsic_TDerrors = [] #list of all the delta, used to uopdate the critic\n",
    "        extrinsic_discounts = []\n",
    "        intrinsic_discounts = []\n",
    "        \n",
    "        for trajectory in self.trajectories:\n",
    "            \n",
    "            advantage_trajectory = [] #list of advantages for each element in a single trajectory\n",
    "            e_delta = []\n",
    "            i_delta = []\n",
    "            e_G = []\n",
    "            i_G = []\n",
    "\n",
    "            v_t = ppo.return_v_extrinsic_values(trajectory[-1][self.STATE])\n",
    "            e_delta.append(trajectory[-2][self.EXTRINSIC_REWARD] + \n",
    "                           self.gamma*v_t - \n",
    "                           ppo.return_v_extrinsic_values(trajectory[-2][self.STATE] ))\n",
    "            e_G.append(trajectory[-2][self.EXTRINSIC_REWARD] + self.gamma*v_t)\n",
    "            e_old_advantage = e_delta[-1]\n",
    "            \n",
    "            v_t = ppo.return_v_intrinsic_values(trajectory[-1][self.STATE])\n",
    "            #normalizing the intrinisc reward before calculating the advantage\n",
    "            i_delta.append( (trajectory[-2][self.INTRINSIC_REWARD] / reward_standard_deviation_estimate) + \n",
    "                           self.gamma*v_t - \n",
    "                           ppo.return_v_intrinsic_values(trajectory[-2][self.STATE] ))\n",
    "            i_G.append(trajectory[-2][self.INTRINSIC_REWARD] + self.gamma*v_t)\n",
    "            i_old_advantage = i_delta[-1]\n",
    "            \n",
    "            advantage_trajectory.append(e_old_advantage[0] + i_old_advantage[0])\n",
    "            \n",
    "            \n",
    "            for i in range(len(trajectory)-3,-1,-1):    \n",
    "                e_delta.append(trajectory[i+1][self.EXTRINSIC_REWARD] + \n",
    "                               self.gamma*ppo.return_v_extrinsic_values(trajectory[i+1][self.STATE]) -  \n",
    "                               ppo.return_v_extrinsic_values(trajectory[i][self.STATE]))\n",
    "                e_G.append(trajectory[i][self.EXTRINSIC_REWARD] + self.gamma*e_G[-1])\n",
    "                new_advantage = e_delta[-1] + self.gamma*self.e_lambda_p*e_old_advantage                                    \n",
    "                                                        \n",
    "                e_old_advantage = new_advantage\n",
    "                \n",
    "                normalized_intrinsic_reward = trajectory[i][self.INTRINSIC_REWARD] / reward_standard_deviation_estimate\n",
    "                i_delta.append(normalized_intrinsic_reward + \n",
    "                               self.gamma*ppo.return_v_intrinsic_values(trajectory[i+1][self.STATE]) -  \n",
    "                               ppo.return_v_intrinsic_values(trajectory[i][self.STATE]))\n",
    "                i_G.append(normalized_intrinsic_reward + self.gamma*i_G[-1])\n",
    "                new_advantage = i_delta[-1] + self.gamma*self.i_lambda_p*i_old_advantage                                    \n",
    "                                                        \n",
    "                i_old_advantage = new_advantage                                        \n",
    "                \n",
    "                advantage_trajectory.append(0.4*i_old_advantage[0] + 0.6*e_old_advantage[0])  \n",
    "        \n",
    "                                                                                                                                                                       \n",
    "            extrinsic_TDerrors.append(e_delta)\n",
    "            intrinsic_TDerrors.append(i_delta)\n",
    "            \n",
    "            extrinsic_discounts.append(e_G)\n",
    "            intrinsic_discounts.append(i_G)\n",
    "                           \n",
    "            advantages.append(advantage_trajectory)\n",
    "            \n",
    "        #flat all trajectories in a single deque adding the advantages (easier to sample random batches)\n",
    "        self.flat_trajectories(self.trajectories, advantages, extrinsic_TDerrors, intrinsic_TDerrors, extrinsic_discounts, intrinsic_discounts)\n",
    "    \n",
    "    def flat_trajectories(self, trajectories, advantages, extrinsic_TDerrors, intrinsic_TDerrors, extrinsic_G, intrinsic_G):\n",
    "        \n",
    "        size = 0\n",
    "        for trajectory in trajectories:\n",
    "            size = size + len(trajectory)\n",
    "        \n",
    "        self.flatten_trajectories = deque(maxlen=size)\n",
    "        \n",
    "        for trajectory, advantage, e_delta, i_delta, e_discount, i_discount in zip(trajectories, advantages, extrinsic_TDerrors, intrinsic_TDerrors, extrinsic_G, intrinsic_G):\n",
    "            for i in range(len(trajectory)-2,-1,-1):\n",
    "                self.flatten_trajectories.append((trajectory[i][self.STATE], \n",
    "                                                  trajectory[i][self.ACTION], \n",
    "                                                  trajectory[i][self.ACTION_PROB], \n",
    "                                                  trajectory[i][self.EXTRINSIC_REWARD], \n",
    "                                                  trajectory[i][self.INTRINSIC_REWARD], \n",
    "                                                  advantage[len(trajectory)-2-i], \n",
    "                                                  e_delta[len(trajectory)-2-i], \n",
    "                                                  i_delta[len(trajectory)-2-i], \n",
    "                                                  e_discount[len(trajectory)-2-i], \n",
    "                                                  i_discount[len(trajectory)-2-i], \n",
    "                                                  trajectory[i][self.DONE]))\n",
    "        \n",
    "        \n",
    "    #pick a random batch example from the flatten list of trajectories\n",
    "    def sample_experiences(self, batch_size):\n",
    "        if (len(self.flatten_trajectories) >= batch_size):\n",
    "            indices = np.random.permutation(len(self.flatten_trajectories))[:batch_size]\n",
    "        else:\n",
    "            indices = np.random.permutation(len(self.flatten_trajectories))\n",
    "        batch = [self.flatten_trajectories[index] for index in indices]\n",
    "        #delete form the memory the used obervations\n",
    "        for index in sorted(indices, reverse=True):\n",
    "            del self.flatten_trajectories[index]\n",
    "        states, actions, actions_prob, e_rewards, i_rewards, advantages, e_TDerrors, i_TDerrors, e_discounts, i_discounts, dones = [np.array([experience[field_index] for experience in batch]) for field_index in range(11)]\n",
    "        return states, actions, actions_prob, e_rewards, i_rewards, advantages, e_TDerrors, i_TDerrors, e_discounts, i_discounts, dones\n",
    "        \n",
    "    def reset(self):\n",
    "        for trajectory in self.trajectories:\n",
    "            trajectory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e6bd4",
   "metadata": {},
   "source": [
    "# RND class\n",
    "\n",
    "$ s_{t+1} $ is the observed state after the current action $ a_t $ \n",
    "\n",
    "Predictor update:\n",
    "minimize $ \\hat{f}(x, \\theta) - f(x) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b20718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RND(object):\n",
    "    \n",
    "    input_shape = [1,84,84] \n",
    "    n_outputs = 200\n",
    "    \n",
    "    N_intrinsic_rewards = 0 #number of intrinsic reward received\n",
    "    intrinisc_reward_mean = 0.0 #mean of the intrinsic rewards received\n",
    "    reward_M2 = 0.0 #sum of squares of differences from the current mean\n",
    "    \n",
    "    N_observations = 0 #number of observations received\n",
    "    observations_mean = 0.0 #mean of the observations received\n",
    "    observation_M2 = 0.0 #sum of squares of differences from the current mean\n",
    "    \n",
    "    def __init__(self, env, n_normalization_steps = 40):\n",
    "        self.target = self.create_target()\n",
    "        self.predictor = self.create_predictor()\n",
    "        \n",
    "        self.MSE = tf.keras.losses.mean_squared_error\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        self.initialize_standard_deviation_estimate(env, n_normalization_steps)\n",
    "        \n",
    "    #create the NN of the target\n",
    "    def create_target(self):\n",
    "        target = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(self.n_outputs) ])\n",
    "        return target\n",
    "        \n",
    "    #create the NN of the predictor\n",
    "    def create_predictor(self):\n",
    "        predictor = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(self.n_outputs) ])\n",
    "        return predictor\n",
    "    \n",
    "    def train_predictor(self, observations):\n",
    "        # extrinsic critic (rewards from the envirnoment)\n",
    "        observations = np.array(observations)\n",
    "        observations = self.normalize_observations(observations)\n",
    "        # covert shape [BATCH_SIZE, 4, 84, 84] in [BATCH_SIZE,1,84,84]\n",
    "        observations = [observation[-1,0:observation.shape[1], 0:observation.shape[2]] for observation in observations]\n",
    "        observations = tf.expand_dims(observations, axis = 1)\n",
    "        target_values = self.target.predict(observations)\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_values = self.predictor(observations)\n",
    "            loss = tf.reduce_mean(self.MSE(target_values, all_values))\n",
    "        grads = tape.gradient(loss, self.predictor.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.predictor.trainable_variables))\n",
    "        \n",
    "    def calculate_intrinsic_reward(self, observation):\n",
    "        #passing a (4,84,84) stacked frame from the environment\n",
    "        observation = np.array(observation)\n",
    "        #picking the last frame\n",
    "        observation = observation[-1, 0:observation.shape[1], 0:observation.shape[2]]\n",
    "        #normalize the last frame\n",
    "        s = self.calculate_observation_standard_deviation()\n",
    "        observation = self.normalize_observation(observation, s)\n",
    "        #calculate intrinsic reward on the last frame\n",
    "        observation = tf.expand_dims(observation, axis=0)\n",
    "        f_target = self.target.predict(tf.expand_dims(observation, axis=0))\n",
    "        f_predictor = self.predictor.predict(tf.expand_dims(observation, axis=0))\n",
    "        #return self.MSE(f_target, f_predictor)*10000\n",
    "        return pow(np.linalg.norm(f_predictor - f_target), 2)*100\n",
    "    \n",
    "    def initialize_standard_deviation_estimate(self, env, n_normalization_steps):\n",
    "        observations_mean = np.zeros(env.observation_space.shape ,'float64') #mean of the intrinsic rewards received\n",
    "        observation_M2 = np.zeros(env.observation_space.shape ,'float64') #sum of squares of differences from the current mean\n",
    "        \n",
    "        obsevation = env.reset()\n",
    "        \n",
    "        for i_step in range(n_normalization_steps):\n",
    "            random_action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(random_action)\n",
    "            for frame in observation:\n",
    "                self.update_observation_normalization_param(frame)\n",
    "    \n",
    "    def update_observation_normalization_param(self, observation):\n",
    "        #cicle trhough the 4 images that makes up for an observation\n",
    "        for obs in observation:\n",
    "            #obs_mean = np.mean([obs_dim], axis=0)\n",
    "            self.N_observations = self.N_observations + 1\n",
    "            delta = obs - self.observations_mean\n",
    "            self.observations_mean = self.observations_mean + delta/self.N_observations # mean_N = mean_{N-1} + (obs_t - mean_{N-1}) / N\n",
    "            self.observation_M2 = self.observation_M2 + delta*(obs - self.observations_mean)\n",
    "        \n",
    "    def calculate_observation_standard_deviation(self):\n",
    "        standard_deviation = np.sqrt( self.observation_M2 / (self.N_observations - 1))\n",
    "        return standard_deviation\n",
    "    \n",
    "    def normalize_observations(self, observations):\n",
    "\n",
    "        norm_obs = []\n",
    "        s = self.calculate_observation_standard_deviation()\n",
    "        for observation in observations:\n",
    "            norm_obs.append(self.normalize_observation(observation, s))\n",
    "        normalized_obs = tf.stack([norm_obs[i] for i in range(len(norm_obs))], 0)\n",
    "\n",
    "        return normalized_obs\n",
    "    \n",
    "    def normalize_observation(self, observation, standard_deviation):\n",
    "        t = observation - self.observations_mean\n",
    "        normalized_obs = np.clip(np.divide(t, standard_deviation, out=np.zeros_like(t), where=standard_deviation!=0), a_min =-5, a_max = 5)       \n",
    "        return normalized_obs\n",
    "    \n",
    "    #def normalize_observation(self, observation, standard_deviation):\n",
    "    #    temp_obs = []\n",
    "    #    for obs_dim in observation:\n",
    "    #        t = obs_dim - self.observations_mean\n",
    "    #        temp_obs.append(np.clip(np.divide(t, standard_deviation, out=np.zeros_like(t), where=standard_deviation!=0), a_min =-5, a_max = 5))\n",
    "    #    normalized_obs = tf.squeeze(tf.stack([temp_obs[i] for i in range(len(temp_obs))], axis = 0))\n",
    "    #    return normalized_obs\n",
    "    \n",
    "    #Using welford's algorithm\n",
    "    def update_reward_normalization_param(self, i_reward):\n",
    "        self.N_intrinsic_rewards = self.N_intrinsic_rewards + 1\n",
    "        delta = i_reward - self.intrinisc_reward_mean\n",
    "        self.intrinisc_reward_mean = self.intrinisc_reward_mean + delta/self.N_intrinsic_rewards # mean_N = mean_{N-1} + (i_t - mean_{N-1}) / N\n",
    "        self.reward_M2 = self.reward_M2 + delta*(i_reward - self.intrinisc_reward_mean)\n",
    "        \n",
    "    def calculate_reward_standard_deviation(self):\n",
    "        standard_deviation = math.sqrt( self.reward_M2 / (self.N_intrinsic_rewards - 1))\n",
    "        print(\"===============================================================\")\n",
    "        print(\"===============================================================\")\n",
    "        print(\"STANDARD DEVIATION {}\".format(standard_deviation))\n",
    "        print(\"===============================================================\")\n",
    "        print(\"===============================================================\")\n",
    "        return standard_deviation\n",
    "    \n",
    "    def save(self, path = \".\\\\saved_weights\\\\rnd\\\\\"):\n",
    "        self.target.save_weights(path + 'target_weights.h5')\n",
    "        self.predictor.save_weights(path + 'predictor_weights.h5')\n",
    "        \n",
    "    def load(self, path = \".\\\\saved_weights\\\\rnd\\\\\"):\n",
    "        self.target.load_weights(path + 'target_weights.h5')\n",
    "        self.predictor.load_weights(path + 'predictor_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2077a580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#basic_env = gym.make(\"GravitarNoFrameskip-v4\", obs_type = \"image\")\n",
    "#wrapped_env = AtariPreprocessing(basic_env)\n",
    "#env = FrameStack(wrapped_env, 4)\n",
    "\n",
    "#rnd = RND(env)\n",
    "\n",
    "#env.reset()\n",
    "\n",
    "#obs = []\n",
    "#for _ in range(32):\n",
    "#    random_action = env.action_space.sample()\n",
    "#    new_obs, reward, done, info = env.step(random_action)\n",
    "#    obs.append(new_obs)\n",
    "    \n",
    "#array = np.array(obs)\n",
    "#rnd.train_predictor(array)\n",
    "\n",
    "#i = rnd.calculate_intrinsic_reward(new_obs)\n",
    "\n",
    "#print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875cd80",
   "metadata": {},
   "source": [
    "# PPO class\n",
    "\n",
    "$ s_{t+1} $ is the observed state after the current action $ a_t $ \n",
    "\n",
    "Actor update formula:\n",
    "$ \\theta_{t+1} = \\theta_t + \\alpha\\nabla min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t)$\n",
    "\n",
    "Critic update formula:\n",
    "$ w_{t+1} = w_t + \\alpha\\delta_t\\nabla\\hat{v}(s_t,w)$\n",
    "\n",
    "Probability ratio $ r_t(\\theta) \\doteq $\n",
    "$ \\pi_\\theta(a_t | s_t) \\over \\pi_{\\theta_{old}}(a_t | s_t) $\n",
    "\n",
    "Advantage:\n",
    "$ \\hat{A}_t \\doteq \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + (\\gamma\\lambda)^2\\delta_{t+2} + ... + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1} = \\delta_t + (\\gamma\\lambda)\\hat{A}_{t+1}$\n",
    "\n",
    "TDerror:\n",
    "$ \\quad \\delta_t  \\doteq $\n",
    "$ r_t + \\gamma\\hat{v}(s_{t+1},w) - \\hat{v}(s_t,w) $ $ \\qquad $ (if $ s_{t+1} $ is terminal then $ \\hat{v}(s_{t+1},w) = 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b137fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(object):\n",
    "    \n",
    "    input_shape = [4,84,84] \n",
    "    n_outputs = 4 #6 #wrapped_env.action_space.n\n",
    "    \n",
    "    next_reward = 0\n",
    "    \n",
    "    def __init__(self, env, n_episodes = 1, train_steps = 100, epsilon = 0.2, alpha = 1, gamma = 0.4, e_lambda_par = 1, i_lambda_par = 1, n_normalization_steps = 300, train_predictor_keeping_prob = 0.25):\n",
    "        self.actor = self.create_actor()\n",
    "        self.intrinsic_critic = self.create_critic()\n",
    "        self.extrinsic_critic = self.create_critic()\n",
    "        \n",
    "        self.MSE = tf.keras.losses.mean_squared_error\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4)\n",
    "        self.extrinsic_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        self.intrinsic_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        #self.n_outputs = 6 #env.action_space.n\n",
    "        \n",
    "        self.train_steps = train_steps\n",
    "        self.train_predictor_keeping_prob = train_predictor_keeping_prob\n",
    "        \n",
    "        self.memory = Memory(n_episodes, gamma, e_lambda_par, i_lambda_par)\n",
    "        \n",
    "        self.rnd = RND(env, n_normalization_steps)\n",
    "        \n",
    "    #create the NN of the actor\n",
    "    # Given the state returns the probability of each action\n",
    "    def create_actor(self):\n",
    "        initializer = tf.keras.initializers.GlorotNormal()\n",
    "        actor = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"tanh\", input_shape = self.input_shape, kernel_initializer=initializer),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"tanh\", kernel_initializer=initializer),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"tanh\", kernel_initializer=initializer),\n",
    "            keras.layers.Dense(512, kernel_initializer=initializer),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(self.n_outputs, kernel_initializer=initializer, activation = 'softmax') ])\n",
    "        return actor\n",
    "       \n",
    "    #create the NN of the critic\n",
    "    # Given the state returns the value function\n",
    "    def create_critic(self):\n",
    "        critic = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(1) ])\n",
    "        \n",
    "        return critic\n",
    "      \n",
    "    def play_one_step(self, env, observation, i_episode):\n",
    "        action, action_prob = self.select_action(observation)\n",
    "        past_reward = self.next_reward\n",
    "        next_observation, self.next_reward, done, info = env.step(action)\n",
    "        \n",
    "        #self.next_reward = float(self.next_reward) / 100\n",
    "        \n",
    "        #the normalization of the intrinisc reward after before training\n",
    "        i_reward = self.rnd.calculate_intrinsic_reward(observation)\n",
    "        \n",
    "        self.rnd.update_reward_normalization_param(i_reward)\n",
    "        self.rnd.update_observation_normalization_param(observation)\n",
    "        \n",
    "        self.memory.collect(observation, action, action_prob, past_reward, i_reward, done, i_episode)\n",
    "        if (done):\n",
    "            self.memory.collect(next_observation, action, action_prob, self.next_reward, i_reward, done, i_episode)\n",
    "            \n",
    "        return next_observation, action, past_reward, i_reward, done, info\n",
    "\n",
    "        \n",
    "    #select the action (returned as a number)\n",
    "    def select_action(self, observation):\n",
    "        \n",
    "        action_probabilities = self.actor.predict(tf.expand_dims(np.array(observation) / 255, axis=0))[0]\n",
    "        action = np.random.choice(a = len(action_probabilities), p = action_probabilities)\n",
    "        \n",
    "        return action, action_probabilities[action]\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        self.memory.calculate_advantages(self.rnd.calculate_reward_standard_deviation())\n",
    "        \n",
    "        for i_step in range(self.train_steps):\n",
    "            done = self.training_step(batch_size)\n",
    "            if (done):\n",
    "                break\n",
    "        \n",
    "        self.memory.reset()\n",
    "        \n",
    "    #training done on the memory (the advantages must be calculated before hand)\n",
    "    def training_step(self, batch_size):\n",
    "        #get experiences (parts of a trajectory) from the memory\n",
    "        \n",
    "        states, actions, actions_prob, extrinsic_rewards, intrinsic_rewards, advantages, extrinsic_TDerrors, intrinsic_TDerrors, extrinsic_discounts, intrinsic_discounts, dones = self.memory.sample_experiences(batch_size)\n",
    "        \n",
    "        done = False\n",
    "        if (len(states) == 0):\n",
    "            return True\n",
    "        if (len(states) != batch_size):\n",
    "            done = True\n",
    "\n",
    "        #compute the values for the update of the actor\n",
    "        \n",
    "        mask = tf.one_hot(actions, self.n_outputs)\n",
    "        \n",
    "        states = np.array(states)\n",
    "        \n",
    "        og_states = states\n",
    "        states = states/255\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            current_actions_prob = self.actor(states)\n",
    "            \n",
    "            current_action_prob = tf.reduce_sum(current_actions_prob*mask, axis=1, keepdims=True)\n",
    "            old_actions_prob = tf.reshape(tf.convert_to_tensor(actions_prob), [len(states), 1])\n",
    "            probability_ratio = tf.divide(tf.math.log(current_action_prob + 1e-7), tf.math.log(old_actions_prob + 1e-7 ))\n",
    "        \n",
    "            #sobtitute nan values with zero (where given an array of True/false puy the element of the first array (tf.zeros_like(probability_ratio)) in the position where is True, the second (probability_ratio) where is False)\n",
    "            probability_ratio = tf.where(tf.math.is_nan(probability_ratio), tf.zeros_like(probability_ratio), probability_ratio)\n",
    "        \n",
    "            surrogate_arg_1 = tf.convert_to_tensor([probability_ratio[index]*advantages[index] for index in range(len(advantages))])\n",
    "            surrogate_arg_2 = tf.convert_to_tensor(np.array([tf.keras.backend.clip(probability_ratio,1-self.epsilon,1+self.epsilon)[index]*advantages[index] for index in range(len(advantages))]).flatten())\n",
    "            \n",
    "            L = 0 - tf.minimum( surrogate_arg_1 , surrogate_arg_2 ) \n",
    "            loss = tf.reduce_mean(L)\n",
    "\n",
    "        actor_weights = self.actor.trainable_variables\n",
    "        grads = tape.gradient(loss, actor_weights)\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, actor_weights))\n",
    "        \n",
    "        #update of the critic. The target is the TD error\n",
    "        \n",
    "        # extrinsic critic (rewards from the envirnoment)\n",
    "        target_v_values = tf.reshape(tf.convert_to_tensor(np.asarray(self.alpha*extrinsic_discounts).astype('float32')), (len(states), 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_values = self.extrinsic_critic(states)\n",
    "            loss = tf.reduce_mean(self.MSE(target_v_values, v_values))\n",
    "        grads = tape.gradient(loss, self.extrinsic_critic.trainable_variables)\n",
    "        self.extrinsic_optimizer.apply_gradients(zip(grads, self.extrinsic_critic.trainable_variables))\n",
    "        \n",
    "        # intrinsic critic (rewards from the exploration)\n",
    "        target_v_values = tf.reshape(tf.convert_to_tensor(np.asarray(self.alpha*intrinsic_discounts).astype('float32')), (len(states), 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_values = self.intrinsic_critic(states)\n",
    "            loss = tf.reduce_mean(self.MSE(target_v_values, v_values))\n",
    "        grads = tape.gradient(loss, self.intrinsic_critic.trainable_variables)\n",
    "        self.intrinsic_optimizer.apply_gradients(zip(grads, self.intrinsic_critic.trainable_variables))\n",
    "        \n",
    "        #since v changed we need to re-calculate the advantages\n",
    "        #self.memory.calculate_advantages()\n",
    "        \n",
    "        keep = random.random()\n",
    "        if (keep <= self.train_predictor_keeping_prob):\n",
    "            self.rnd.train_predictor(og_states)\n",
    "        \n",
    "        return done\n",
    "    \n",
    "    def return_v_extrinsic_values(self, observation):\n",
    "        v_e = self.extrinsic_critic.predict(tf.expand_dims(np.array(observation) / 255, axis=0))[0]\n",
    "        return v_e\n",
    "    \n",
    "    def return_v_intrinsic_values(self, observation):\n",
    "        v_i = self.intrinsic_critic.predict(tf.expand_dims(np.array(observation) / 255, axis=0))[0]\n",
    "        return v_i\n",
    "    \n",
    "    def save(self, path = \".\\\\saved_weights\\\\rnd\\\\\"):\n",
    "        self.actor.save_weights(path + 'actor_weights.h5')\n",
    "        self.extrinsic_critic.save_weights(path + 'e_critic_weights.h5')\n",
    "        self.intrinsic_critic.save_weights(path + 'i_critic_weights.h5')\n",
    "        self.rnd.save()\n",
    "        \n",
    "    def load(self, path = \".\\\\saved_weights\\\\rnd\\\\\"):\n",
    "        self.actor.load_weights(path + 'actor_weights.h5')\n",
    "        self.extrinsic_critic.load_weights(path + 'e_critic_weights.h5')\n",
    "        self.intrinsic_critic.load_weights(path + 'i_critic_weights.h5')\n",
    "        self.rnd.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2869d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a4f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class collect_trajectory(Thread):\n",
    "    \n",
    "    def __init__(self, env, i_agent):\n",
    "         \n",
    "        Thread.__init__(self)   \n",
    "        self.n_agent = i_agent\n",
    "        self.rewards = [] \n",
    "        \n",
    "        self.env = env\n",
    "            \n",
    "    def run(self):\n",
    "        #print(\"Starting {}\".format(self.n_agent))\n",
    "        \n",
    "        observation = self.env.reset()\n",
    "        \n",
    "        self.env.step(1)\n",
    "        past_lives = 5\n",
    "        \n",
    "        extrinsic_episode_reward = 0.0\n",
    "        intrinsic_episode_reward = 0.0\n",
    "        self.n_episodes = 0.0\n",
    "        self.extrinsic_tot_reward = 0.0\n",
    "        self.intrinsic_tot_reward = 0.0\n",
    "        \n",
    "        for i_step in range(N_STEPS):   \n",
    "            observation, action, extrinsic_reward, intrinsic_reward, done, info = ppo.play_one_step(self.env, observation, self.n_agent)\n",
    "\n",
    "            current_lives = info['ale.lives']\n",
    "    \n",
    "            if (current_lives <= past_lives):\n",
    "                past_lives = current_lives\n",
    "                observation, reward, done, info = self.env.step(1)\n",
    "            \n",
    "            \n",
    "            extrinsic_episode_reward = extrinsic_episode_reward + extrinsic_reward\n",
    "            intrinsic_episode_reward = intrinsic_episode_reward + intrinsic_reward\n",
    "            \n",
    "            #continuing task. if an episode is done we continue until complting the number of steps\n",
    "            if (done):\n",
    "                observation = self.env.reset()\n",
    "                \n",
    "                past_lives = 5\n",
    "                \n",
    "                self.extrinsic_tot_reward = self.extrinsic_tot_reward + extrinsic_episode_reward\n",
    "                self.intrinsic_tot_reward = self.intrinsic_tot_reward + intrinsic_episode_reward\n",
    "                self.n_episodes = self.n_episodes + 1\n",
    "                episode_reward = 0.0\n",
    "                \n",
    "        if (self.n_episodes == 0):\n",
    "            self.extrinsic_tot_reward = extrinsic_episode_reward\n",
    "            self.intrinsic_tot_reward = intrinsic_episode_reward\n",
    "        \n",
    "        self.env.close()\n",
    "        \n",
    "        if (self.n_episodes > 0):\n",
    "            print(\"Exiting {} after {} episodes. Average ex reward: {} in reward: {}\".format(self.n_agent, self.n_episodes, self.extrinsic_tot_reward/self.n_episodes, self.intrinsic_tot_reward/self.n_episodes))\n",
    "        else:\n",
    "            print(\"Exiting {} after {} episodes. Average ex reward: {} in reward: {}\".format(self.n_agent, self.n_episodes, self.extrinsic_tot_reward, self.intrinsic_tot_reward))\n",
    "    \n",
    "    \n",
    "    def get_reward_average(self):\n",
    "        if (self.n_episodes > 0):\n",
    "            return (self.extrinsic_tot_reward/self.n_episodes, self.intrinsic_tot_reward/self.n_episodes)\n",
    "        else:\n",
    "            return (self.extrinsic_tot_reward, self.intrinsic_tot_reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700dfc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting 0 after 22.0 episodes. Average ex reward: 5.818181818181818 in reward: 6490.062065049034\n",
      "Exiting 1 after 24.0 episodes. Average ex reward: 7.75 in reward: 6484.155351956982\n",
      "Epoch: 0 ended with average extrinsic reward: 6.784090909090909 intrinsic reward 6487.108708503008 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 2.037980205385447\n",
      "===============================================================\n",
      "===============================================================\n",
      "Exiting 1 after 15.0 episodes. Average ex reward: 11.666666666666666 in reward: 337.3550634486901\n",
      "Exiting 0 after 15.0 episodes. Average ex reward: 10.0 in reward: 358.93819310097274\n",
      "Epoch: 1 ended with average extrinsic reward: 10.833333333333332 intrinsic reward 348.1466282748314 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 2.908934785908415\n",
      "===============================================================\n",
      "===============================================================\n",
      "Exiting 1 after 17.0 episodes. Average ex reward: 7.294117647058823 in reward: 31.005947957958462\n",
      "Exiting 0 after 16.0 episodes. Average ex reward: 8.75 in reward: 28.413653557468265\n",
      "Epoch: 2 ended with average extrinsic reward: 8.022058823529411 intrinsic reward 29.709800757713364 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 2.719347251335083\n",
      "===============================================================\n",
      "===============================================================\n",
      "Exiting 0 after 16.0 episodes. Average ex reward: 11.625 in reward: 40.110686946000826\n",
      "Exiting 1 after 16.0 episodes. Average ex reward: 12.9375 in reward: 36.433874274806556\n",
      "Epoch: 3 ended with average extrinsic reward: 12.28125 intrinsic reward 38.27228061040369 \n",
      "\n",
      "===============================================================\n",
      "===============================================================\n",
      "STANDARD DEVIATION 2.4901972177322453\n",
      "===============================================================\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "N_STEPS_NORMALIZATION = 50\n",
    "N_EPOCHS = 100\n",
    "N_EPISODES = 2 # in multi-agent this is the number of agents (each agnet collect 1 trajectory)\n",
    "N_STEPS = 2000 # max number of step for each episode\n",
    "\n",
    "TRAIN_STEPS = 300 # number of max steps done during training. if the number of samples is less than TRAIN_STEPS*BATCH_SIZE will stop early after completing the training on all the samples\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "environment = \"BreakoutNoFrameskip-v4\" #\"GravitarNoFrameskip-v4\", \"PongNoFrameskip-v4\", \"AirRaidNoFrameskip-v4\"\n",
    "\n",
    "#env used to initialize the parameters inside PPO and RND\n",
    "norm_env = gym.make(environment, obs_type = \"image\")\n",
    "norm_wrapped_env = AtariPreprocessing(norm_env)\n",
    "norm_stack_env = FrameStack(norm_wrapped_env, 4)\n",
    "\n",
    "ppo = PPO(norm_stack_env, n_episodes = N_EPISODES, train_steps = TRAIN_STEPS, n_normalization_steps = N_STEPS_NORMALIZATION, i_lambda_par = 2.25, train_predictor_keeping_prob = 0.6)\n",
    "\n",
    "e_rewards = []\n",
    "i_rewards = []\n",
    "\n",
    "envs = []\n",
    "\n",
    "for i_env in range(N_EPISODES):\n",
    "        basic_env = gym.make(environment, obs_type = \"image\")\n",
    "        wrapped_env = AtariPreprocessing(basic_env)\n",
    "        envs.append(FrameStack(wrapped_env, 4))\n",
    "\n",
    "highest_average_reward = 0\n",
    "        \n",
    "for i_epoch in range(N_EPOCHS):\n",
    "    extrinsic_epoch_reward = 0.0\n",
    "    intrinsic_epoch_reward = 0.0\n",
    "    agents = []\n",
    "    for i_agent in range(N_EPISODES):\n",
    "        agents.append(collect_trajectory(env = envs[i_agent], i_agent = i_agent))\n",
    "    for agent in agents:\n",
    "        agent.start()\n",
    "    for agent in agents:\n",
    "        agent.join()\n",
    "        extrinsic_reward_average, intrinsic_reward_average = agent.get_reward_average()\n",
    "        extrinsic_epoch_reward = extrinsic_epoch_reward + extrinsic_reward_average\n",
    "        intrinsic_epoch_reward = intrinsic_epoch_reward + intrinsic_reward_average\n",
    "    e_rewards.append(extrinsic_epoch_reward/N_EPISODES)\n",
    "    i_rewards.append(intrinsic_epoch_reward/N_EPISODES)\n",
    "    print(\"Epoch: {} ended with average extrinsic reward: {} intrinsic reward {} \\n\".format(i_epoch, extrinsic_epoch_reward/N_EPISODES, intrinsic_epoch_reward/N_EPISODES) )  \n",
    "    \n",
    "    if (highest_average_reward <= e_rewards[-1]):\n",
    "        highest_average_reward = e_rewards[-1]\n",
    "        ppo.save()\n",
    "    \n",
    "    ppo.train(batch_size = BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "for i_env in range(N_EPISODES):\n",
    "    envs[i_env].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7093c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gym.wrappers.frame_stack.LazyFrames object at 0x000001E405E99770>\n",
      "Starting demo\n",
      "selected action 2 with prob 0.9751538634300232 got reward 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Impossibile cambiare la modalità del thread dopo averla impostata\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected action 2 with prob 0.9750579595565796 got reward 0.0\n",
      "selected action 2 with prob 0.9753133654594421 got reward 0.0\n",
      "selected action 2 with prob 0.9747897982597351 got reward 0.0\n",
      "selected action 2 with prob 0.974651575088501 got reward 0.0\n",
      "selected action 2 with prob 0.9752400517463684 got reward 0.0\n",
      "selected action 2 with prob 0.9750487804412842 got reward 0.0\n",
      "selected action 2 with prob 0.9751133918762207 got reward 0.0\n",
      "selected action 2 with prob 0.9750691056251526 got reward 0.0\n",
      "selected action 2 with prob 0.9751991033554077 got reward 0.0\n",
      "selected action 2 with prob 0.9753149747848511 got reward 0.0\n",
      "selected action 1 with prob 0.024170784279704094 got reward 0.0\n",
      "selected action 2 with prob 0.9749774932861328 got reward 0.0\n",
      "selected action 2 with prob 0.9750741720199585 got reward 0.0\n",
      "selected action 2 with prob 0.9751831293106079 got reward 0.0\n",
      "selected action 2 with prob 0.9749694466590881 got reward 0.0\n",
      "selected action 2 with prob 0.9750814437866211 got reward 0.0\n",
      "selected action 2 with prob 0.9752055406570435 got reward 0.0\n",
      "selected action 2 with prob 0.9751763939857483 got reward 0.0\n",
      "selected action 2 with prob 0.9751628041267395 got reward 0.0\n",
      "selected action 2 with prob 0.9751129150390625 got reward 0.0\n",
      "selected action 2 with prob 0.9751164317131042 got reward 0.0\n",
      "selected action 2 with prob 0.9751164317131042 got reward 0.0\n",
      "selected action 2 with prob 0.9751164317131042 got reward 0.0\n",
      "selected action 2 with prob 0.9751564860343933 got reward 0.0\n",
      "selected action 2 with prob 0.9750653505325317 got reward 0.0\n",
      "selected action 2 with prob 0.9750201106071472 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 1 with prob 0.024265097454190254 got reward 0.0\n",
      "selected action 2 with prob 0.9750254154205322 got reward 0.0\n",
      "selected action 2 with prob 0.9749153852462769 got reward 0.0\n",
      "selected action 2 with prob 0.9751418232917786 got reward 0.0\n",
      "selected action 2 with prob 0.9746862649917603 got reward 0.0\n",
      "selected action 2 with prob 0.9747591018676758 got reward 0.0\n",
      "selected action 2 with prob 0.9751176834106445 got reward 0.0\n",
      "selected action 1 with prob 0.024328026920557022 got reward 0.0\n",
      "selected action 2 with prob 0.9749694466590881 got reward 0.0\n",
      "selected action 2 with prob 0.9749249219894409 got reward 0.0\n",
      "selected action 2 with prob 0.9750555753707886 got reward 0.0\n",
      "selected action 2 with prob 0.9751721024513245 got reward 0.0\n",
      "selected action 2 with prob 0.9749258160591125 got reward 0.0\n",
      "selected action 2 with prob 0.9748328328132629 got reward 0.0\n",
      "selected action 2 with prob 0.9749300479888916 got reward 0.0\n",
      "selected action 2 with prob 0.9750394821166992 got reward 0.0\n",
      "selected action 2 with prob 0.9748246669769287 got reward 0.0\n",
      "selected action 2 with prob 0.9749372601509094 got reward 0.0\n",
      "selected action 2 with prob 0.9750621914863586 got reward 0.0\n",
      "selected action 2 with prob 0.9750328063964844 got reward 0.0\n",
      "selected action 2 with prob 0.975019097328186 got reward 0.0\n",
      "selected action 2 with prob 0.9749690294265747 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749725461006165 got reward 0.0\n",
      "selected action 2 with prob 0.9749592542648315 got reward 0.0\n",
      "selected action 2 with prob 0.9750775694847107 got reward 0.0\n",
      "selected action 2 with prob 0.9751009345054626 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 1 with prob 0.024104425683617592 got reward 0.0\n",
      "selected action 2 with prob 0.9751887917518616 got reward 0.0\n",
      "selected action 2 with prob 0.9750794768333435 got reward 0.0\n",
      "selected action 2 with prob 0.9753044247627258 got reward 0.0\n",
      "selected action 2 with prob 0.9748517274856567 got reward 0.0\n",
      "selected action 2 with prob 0.9749240279197693 got reward 0.0\n",
      "selected action 2 with prob 0.9752804040908813 got reward 0.0\n",
      "selected action 2 with prob 0.9750686287879944 got reward 0.0\n",
      "selected action 2 with prob 0.9751332402229309 got reward 0.0\n",
      "selected action 2 with prob 0.975088894367218 got reward 0.0\n",
      "selected action 2 with prob 0.9752187132835388 got reward 0.0\n",
      "selected action 2 with prob 0.9753344655036926 got reward 0.0\n",
      "selected action 2 with prob 0.9750898480415344 got reward 0.0\n",
      "selected action 2 with prob 0.9749974608421326 got reward 0.0\n",
      "selected action 2 with prob 0.9750940203666687 got reward 0.0\n",
      "selected action 2 with prob 0.975202739238739 got reward 0.0\n",
      "selected action 2 with prob 0.9749891757965088 got reward 0.0\n",
      "selected action 2 with prob 0.9751011729240417 got reward 0.0\n",
      "selected action 2 with prob 0.9752251505851746 got reward 0.0\n",
      "selected action 2 with prob 0.9751960039138794 got reward 0.0\n",
      "selected action 2 with prob 0.9751825332641602 got reward 0.0\n",
      "selected action 2 with prob 0.9751326441764832 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 1 with prob 0.024104425683617592 got reward 0.0\n",
      "selected action 2 with prob 0.9751361608505249 got reward 0.0\n",
      "selected action 2 with prob 0.9751113653182983 got reward 0.0\n",
      "selected action 2 with prob 0.9750494956970215 got reward 0.0\n",
      "selected action 2 with prob 0.9749906659126282 got reward 0.0\n",
      "selected action 2 with prob 0.9750097990036011 got reward 0.0\n",
      "selected action 2 with prob 0.9750097990036011 got reward 0.0\n",
      "selected action 2 with prob 0.9750097990036011 got reward 0.0\n",
      "selected action 2 with prob 0.9750097990036011 got reward 0.0\n",
      "selected action 1 with prob 0.0242299847304821 got reward 0.0\n",
      "selected action 2 with prob 0.9750626087188721 got reward 0.0\n",
      "selected action 2 with prob 0.9749528169631958 got reward 0.0\n",
      "selected action 2 with prob 0.9751787781715393 got reward 0.0\n",
      "selected action 2 with prob 0.9747239947319031 got reward 0.0\n",
      "selected action 2 with prob 0.9747965931892395 got reward 0.0\n",
      "selected action 2 with prob 0.9751547574996948 got reward 0.0\n",
      "selected action 2 with prob 0.9749417901039124 got reward 0.0\n",
      "selected action 2 with prob 0.9750068783760071 got reward 0.0\n",
      "selected action 2 with prob 0.9749622344970703 got reward 0.0\n",
      "selected action 2 with prob 0.9750927686691284 got reward 0.0\n",
      "selected action 2 with prob 0.9752090573310852 got reward 0.0\n",
      "selected action 2 with prob 0.9749632477760315 got reward 0.0\n",
      "selected action 2 with prob 0.9748702049255371 got reward 0.0\n",
      "selected action 2 with prob 0.9749673008918762 got reward 0.0\n",
      "selected action 2 with prob 0.9750766754150391 got reward 0.0\n",
      "selected action 2 with prob 0.9748620390892029 got reward 0.0\n",
      "selected action 2 with prob 0.9749745726585388 got reward 0.0\n",
      "selected action 2 with prob 0.9750992059707642 got reward 0.0\n",
      "selected action 2 with prob 0.9750699996948242 got reward 0.0\n",
      "selected action 2 with prob 0.9750564098358154 got reward 0.0\n",
      "selected action 2 with prob 0.9750061631202698 got reward 0.0\n",
      "selected action 1 with prob 0.0242299847304821 got reward 0.0\n",
      "selected action 2 with prob 0.9750097990036011 got reward 0.0\n",
      "selected action 2 with prob 0.9750097990036011 got reward 0.0\n",
      "selected action 2 with prob 0.9749552011489868 got reward 0.0\n",
      "selected action 2 with prob 0.9749851226806641 got reward 0.0\n"
     ]
    }
   ],
   "source": [
    "basic_env = gym.make(\"BreakoutNoFrameskip-v4\", obs_type = \"image\")\n",
    "wrapped_env = AtariPreprocessing(basic_env)\n",
    "stack_env = FrameStack(wrapped_env, 4)\n",
    "\n",
    "ppo = PPO(env = stack_env)\n",
    "ppo.load()\n",
    "\n",
    "observation = stack_env.reset()\n",
    "observation, reward, done, info = stack_env.step(1)\n",
    "print(observation)\n",
    "print(\"Starting demo\")\n",
    "for i_step in range(200):   \n",
    "    \n",
    "    action, action_prob = ppo.select_action(observation)\n",
    "    \n",
    "    observation, reward, done, info = stack_env.step(action)\n",
    "    #rand_action = stack_env.action_space.sample()\n",
    "    #observation, reward, done, info = stack_env.step(rand_action)\n",
    "    print(\"selected action {} with prob {} got reward {}\".format(action, action_prob, reward))\n",
    "\n",
    "    stack_env.render()\n",
    "            \n",
    "    if (done):\n",
    "        break\n",
    "        \n",
    "stack_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5b122",
   "metadata": {},
   "source": [
    "## Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(205)#N_EPOCHS)\n",
    "\n",
    "plt.plot(epochs, i_rewards)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d710415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
