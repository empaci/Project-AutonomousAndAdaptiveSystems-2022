{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b284cff8",
   "metadata": {},
   "source": [
    "# AirRaid-PPO implementation\n",
    "\n",
    "PPO test in the AirRaid environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d37ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import AtariPreprocessing\n",
    "from gym.wrappers import FrameStack\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from collections import deque\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from threading import Thread\n",
    "\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c963de",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "Class used to memorize the trajectory and calculate the advntage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23b7a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    \n",
    "    STATE = 0\n",
    "    ACTION = 1\n",
    "    ACTION_PROB = 2\n",
    "    REWARD = 3\n",
    "    DONE = 4\n",
    "    \n",
    "    def __init__(self, n_trajectories, gamma = 0.98, lambda_p = 0.96):\n",
    "        self.trajectories = np.empty(n_trajectories, dtype=object)\n",
    "        self.gamma = gamma\n",
    "        self.lambda_p = lambda_p\n",
    "              \n",
    "    def collect(self, state, action, action_prob, reward, done, i_episode):\n",
    "        if (self.trajectories[i_episode] == None):\n",
    "            self.trajectories[i_episode] = deque(maxlen=N_STEPS)\n",
    "        self.trajectories[i_episode].append((state, action, action_prob, reward, done))\n",
    "        \n",
    "    def calculate_advantages(self):\n",
    "        advantages = []\n",
    "        TDerrors = [] #list of all the delta, used to uopdate the critic\n",
    "        discounts = []\n",
    "        \n",
    "        for trajectory in self.trajectories:\n",
    "            \n",
    "            advantage_trajectory = [] #list of advantages for each element in a single trajectory\n",
    "            delta = []\n",
    "            G = []\n",
    "            \n",
    "            delta.append(trajectory[-2][self.REWARD] + self.gamma*ppo.return_v_values(trajectory[-1][self.STATE]) - ppo.return_v_values(trajectory[-2][self.STATE]) ) \n",
    "            old_advantage = delta[-1] \n",
    "            advantage_trajectory.append(old_advantage[0])\n",
    "            G.append(trajectory[-2][self.REWARD] + self.gamma*ppo.return_v_values(trajectory[-1][self.STATE]))\n",
    "\n",
    "            for i in range(len(trajectory)-3,-1,-1):\n",
    "                delta.append(trajectory[i+1][self.REWARD] + self.gamma*ppo.return_v_values(trajectory[i+1][self.STATE]) - ppo.return_v_values(trajectory[i][self.STATE]))\n",
    "                G.append(trajectory[i][self.REWARD] + self.gamma*G[-1])\n",
    "                new_advantage = delta[-1] + self.gamma*self.lambda_p*old_advantage\n",
    "\n",
    "                advantage_trajectory.append(new_advantage[0])   \n",
    "\n",
    "                old_advantage = new_advantage\n",
    "\n",
    "            advantages.append(advantage_trajectory)\n",
    "            TDerrors.append(delta)\n",
    "            discounts.append(G)\n",
    "            \n",
    "        #flat all trajectories in a single deque adding the advantages (easier to sample random batches)\n",
    "        self.flat_trajectories(advantages, TDerrors, discounts)\n",
    "    \n",
    "    def flat_trajectories(self, advantages, TDerrors, G):\n",
    "        \n",
    "        size = 0\n",
    "        for trajectory in self.trajectories:\n",
    "            size = size + len(trajectory)\n",
    "        self.flatten_trajectories = deque(maxlen=size)\n",
    "        \n",
    "        for trajectory, advantage, delta, discount in zip(self.trajectories, advantages, TDerrors, G):\n",
    "            for i in range(len(trajectory)-2, -1, -1):\n",
    "                self.flatten_trajectories.append((trajectory[i][self.STATE], \n",
    "                                                  trajectory[i][self.ACTION], \n",
    "                                                  trajectory[i][self.ACTION_PROB], \n",
    "                                                  trajectory[i][self.REWARD], \n",
    "                                                  advantage[len(trajectory)-2-i], #they are reversed in respect to the trajectory\n",
    "                                                  delta[len(trajectory)-2-i],\n",
    "                                                  discount[len(trajectory)-2-i],\n",
    "                                                  trajectory[i][self.DONE]))\n",
    "                #if (trajectory[i][self.DONE]):\n",
    "                #    print(\"================================================\")\n",
    "                #print(\"{}. v: {} G: {}\".format(i, ppo.return_v_values(trajectory[i][self.STATE]), discount[len(trajectory)-2-i]))\n",
    "                #print(\"Action {}\".format(trajectory[i][self.ACTION]))\n",
    "                #print(\"Action Prob {}\".format(trajectory[i][self.ACTION_PROB]))\n",
    "                #print(\"Advantage {}\".format(advantage[len(trajectory)-2-i]))\n",
    "                #print(\"Delta {}\".format(delta[len(trajectory)-2-i]))\n",
    "        \n",
    "        \n",
    "    #pick a random batch example from the flatten list of trajectories\n",
    "    def sample_experiences(self, batch_size):\n",
    "        if (len(self.flatten_trajectories) >= batch_size):\n",
    "            indices = np.random.permutation(len(self.flatten_trajectories))[:batch_size]\n",
    "        else:\n",
    "            indices = np.random.permutation(len(self.flatten_trajectories))\n",
    "        batch = [self.flatten_trajectories[index] for index in indices]\n",
    "        #delete form the memory the used obervations\n",
    "        for index in sorted(indices, reverse=True):\n",
    "            del self.flatten_trajectories[index]\n",
    "        states, actions, actions_prob, rewards, advantages, TDerrors, discounts, dones = [np.array([experience[field_index] for experience in batch]) for field_index in range(8)]\n",
    "        return states, actions, actions_prob, rewards, advantages, TDerrors, discounts, dones\n",
    "        \n",
    "    def reset(self):\n",
    "        for trajectory in self.trajectories:\n",
    "            trajectory.clear()\n",
    "        self.flatten_trajectories.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875cd80",
   "metadata": {},
   "source": [
    "# PPO class\n",
    "\n",
    "$ s_{t+1} $ is the observed state after the current action $ a_t $ \n",
    "\n",
    "Actor update formula:\n",
    "$ \\theta_{t+1} = \\theta_t + \\alpha\\nabla min(r_t(\\theta)\\hat{A}_t, clip(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t)$\n",
    "\n",
    "Critic update formula:\n",
    "$ w_{t+1} = w_t + \\alpha G_t\\nabla\\hat{v}(s_t,w)$\n",
    "\n",
    "Probability ratio $ r_t(\\theta) \\doteq $\n",
    "$ \\pi_\\theta(a_t | s_t) \\over \\pi_{\\theta_old}(a_t | s_t) $\n",
    "\n",
    "Advantage:\n",
    "$ \\hat{A}_t \\doteq \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + (\\gamma\\lambda)^2\\delta_{t+2} + ... + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1} = \\delta_t + (\\gamma\\lambda)\\hat{A}_{t+1}$\n",
    "\n",
    "TDerror:\n",
    "$ \\quad \\delta_t  \\doteq $\n",
    "$ G_t - \\hat{v}(s_t,w) $ $ \\qquad $ (if $ s_{t+1} $ is terminal then $ \\hat{v}(s_{t+1},w) = 0$)\n",
    "\n",
    "Discounted return:\n",
    "$ G_t \\doteq $\n",
    "$ r_t + \\gamma\\hat{v}(s_{t+1},w) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62b137fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(object):\n",
    "    \n",
    "    input_shape = [4,84,84] \n",
    "    n_outputs = 4 #stack_env.action_space.n\n",
    "    \n",
    "    next_reward = 0\n",
    "    \n",
    "    def __init__(self, n_episodes = 1, train_steps = 100, epsilon = 0.2, alpha = 1):\n",
    "        self.actor = self.create_actor()\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4)\n",
    "        \n",
    "        self.critic = self.create_critic()\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.train_steps = train_steps\n",
    "        \n",
    "        self.memory = Memory(n_episodes)\n",
    "        \n",
    "    #create the NN of the actor\n",
    "    # Given the state returns the probability of each action\n",
    "    def create_actor(self):\n",
    "        initializer = tf.keras.initializers.GlorotNormal()\n",
    "        actor = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"tanh\", input_shape = self.input_shape, kernel_initializer=initializer),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"tanh\", kernel_initializer=initializer),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"tanh\", kernel_initializer=initializer),\n",
    "            keras.layers.Dense(512, kernel_initializer=initializer),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(self.n_outputs, kernel_initializer=initializer, activation = 'softmax') ])\n",
    "        return actor\n",
    "       \n",
    "    #create the NN of the critic\n",
    "    # Given the state returns the value function\n",
    "    def create_critic(self):\n",
    "        critic = keras.Sequential([\n",
    "            keras.layers.Conv2D(filters=32, kernel_size = (8,8), strides=4, activation=\"relu\", input_shape = self.input_shape),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (4,4), strides=2, activation=\"relu\"),\n",
    "            keras.layers.Conv2D(filters=64, kernel_size = (3,3), strides=1, activation=\"relu\"),\n",
    "            keras.layers.Dense(512),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            keras.layers.Dense(1) ])\n",
    "        \n",
    "        self.critic_loss_fn = tf.keras.losses.mean_squared_error\n",
    "        \n",
    "        return critic\n",
    "      \n",
    "    def play_one_step(self, env, observation, i_episode):\n",
    "        action, action_prob = self.select_action(observation)\n",
    "        past_reward = self.next_reward\n",
    "        next_observation, self.next_reward, done, info = env.step(action)\n",
    "        \n",
    "        #self.next_reward = ((float) self.next_reward) / 100\n",
    "        \n",
    "        self.memory.collect(observation, action, action_prob, past_reward, done, i_episode)\n",
    "        if (done):\n",
    "            self.memory.collect(next_observation, action, action_prob, self.next_reward, done, i_episode)\n",
    "            \n",
    "        return next_observation, action, past_reward, done, info\n",
    "        \n",
    "    #select the action\n",
    "    def select_action(self, observation):\n",
    "        \n",
    "        action_probabilities = self.actor.predict(tf.expand_dims(np.array(observation) / 255, axis=0))[0]\n",
    "        action = np.random.choice(a = len(action_probabilities), p = action_probabilities)\n",
    "        \n",
    "        return action, action_probabilities[action]\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        self.memory.calculate_advantages()\n",
    "        \n",
    "        for i_step in range(self.train_steps):\n",
    "            done = self.training_step(batch_size)\n",
    "            if (done):\n",
    "                break\n",
    "        \n",
    "        self.memory.reset()\n",
    "        \n",
    "    #training done on the memory (the advantages must be calculated before hand)\n",
    "    def training_step(self, batch_size):\n",
    "        #get experiences (parts of a trajectory) from the memory\n",
    "        states, actions, actions_prob, rewards, advantages, TDerrors, discounts, dones = self.memory.sample_experiences(batch_size)\n",
    "        \n",
    "        done = False\n",
    "        if (len(states) == 0):\n",
    "            return True\n",
    "        if (len(states) != batch_size):\n",
    "            done = True\n",
    "        \n",
    "        #compute the values for the update of the actor\n",
    "        \n",
    "        mask = tf.one_hot(actions, self.n_outputs)\n",
    "\n",
    "        states = np.array(states) / 255\n",
    "        #array of shape (64,) into array of shape (64,1)\n",
    "        #states =  np.array(np.array_split(states, len(states)))\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            current_actions_prob = self.actor(states)\n",
    "            \n",
    "            current_action_prob = tf.reduce_sum(current_actions_prob*mask, axis=1, keepdims=True)\n",
    "            old_actions_prob = tf.reshape(tf.convert_to_tensor(actions_prob), [len(states), 1])\n",
    "            probability_ratio = tf.divide(tf.math.log(current_action_prob + 1e-7), tf.math.log(old_actions_prob + 1e-7 ))\n",
    "        \n",
    "            #sobtitute nan values with zero (where given an array of True/false puy the element of the first array (tf.zeros_like(probability_ratio)) in the position where is True, the second (probability_ratio) where is False)\n",
    "            probability_ratio = tf.where(tf.math.is_nan(probability_ratio), tf.zeros_like(probability_ratio), probability_ratio)\n",
    "        \n",
    "            surrogate_arg_1 = tf.convert_to_tensor([probability_ratio[index]*advantages[index] for index in range(len(advantages))])\n",
    "            surrogate_arg_2 = tf.convert_to_tensor(np.array([tf.keras.backend.clip(probability_ratio,1-self.epsilon,1+self.epsilon)[index]*advantages[index] for index in range(len(advantages))]).flatten())\n",
    "            \n",
    "            L = 0 - tf.minimum( surrogate_arg_1 , surrogate_arg_2 ) \n",
    "            loss = tf.reduce_mean(L)\n",
    "\n",
    "        actor_weights = self.actor.trainable_variables\n",
    "        grads = tape.gradient(loss, actor_weights)\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, actor_weights))\n",
    "        \n",
    "        #update of the critic. The target is Gt\n",
    "        target_v_values = tf.reshape(tf.convert_to_tensor(np.asarray(self.alpha*discounts).astype('float32')), (len(states), 1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_values = self.critic(states)\n",
    "            loss = tf.reduce_mean(self.critic_loss_fn(target_v_values, v_values))\n",
    "        grads = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))\n",
    "        \n",
    "        return done\n",
    "    \n",
    "    def return_v_values(self, observation):\n",
    "        v = self.critic.predict(tf.expand_dims(np.array(observation) / 255, axis=0))[0]\n",
    "        return v\n",
    "    \n",
    "    def save(self, path = \".\\\\saved_weights\\\\ppo\\\\\"):\n",
    "        self.actor.save_weights(path + 'actor_weights.h5')\n",
    "        self.critic.save_weights(path + 'critic_weights.h5')\n",
    "        \n",
    "    def load(self, path = \".\\\\saved_weights\\\\ppo\\\\\"):\n",
    "        self.actor.load_weights(path + 'actor_weights.h5')\n",
    "        self.critic.load_weights(path + 'critic_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2869d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cdc181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class collect_trajectory(Thread):\n",
    "    \n",
    "    def __init__(self, environment, i_agent, env):\n",
    "         \n",
    "        Thread.__init__(self)   \n",
    "        self.n_agent = i_agent\n",
    "        self.rewards = [] \n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "    def run(self):\n",
    "        \n",
    "        observation = self.env.reset()\n",
    "        past_lifes = 5 #info['ale.lives']\n",
    "        \n",
    "        self.env.step(1)\n",
    "        \n",
    "        self.episode_reward = 0.0\n",
    "        \n",
    "        for i_step in range(N_STEPS):   \n",
    "            observation, action, reward, done, info = ppo.play_one_step(self.env, observation, self.n_agent)\n",
    "\n",
    "            current_lifes = info['ale.lives']\n",
    "    \n",
    "            if (current_lifes <= past_lifes):\n",
    "                past_lifes = current_lifes\n",
    "                observation, reward, done, info = self.env.step(1)\n",
    "            \n",
    "            self.episode_reward = self.episode_reward + reward\n",
    "            \n",
    "            #continuing task. if an episode is done we continue until complting the number of steps\n",
    "            if (done):\n",
    "                break\n",
    "        \n",
    "        self.env.close()\n",
    "        \n",
    "        print(\"Exiting {} afte n. steps {}. Tot reward: {}\".format(self.n_agent, i_step, self.episode_reward))\n",
    "    \n",
    "    def get_reward(self):\n",
    "        return self.episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9a76fea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting 10 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 6 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 8 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 9 afte n. steps 74. Tot reward: 0.0\n",
      "Exiting 1 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 12 afte n. steps 83. Tot reward: 1.0\n",
      "Exiting 0 afte n. steps 74. Tot reward: 0.0\n",
      "Exiting 2 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 14 afte n. steps 83. Tot reward: 1.0\n",
      "Exiting 5 afte n. steps 89. Tot reward: 1.0\n",
      "Exiting 7 afte n. steps 97. Tot reward: 1.0\n",
      "Exiting 4 afte n. steps 97. Tot reward: 1.0\n",
      "Exiting 15 afte n. steps 97. Tot reward: 1.0\n",
      "Exiting 3 afte n. steps 118. Tot reward: 2.0\n",
      "Exiting 13 afte n. steps 132. Tot reward: 2.0\n",
      "Exiting 11 afte n. steps 112. Tot reward: 2.0\n",
      "Epoch: 0 ended with average reward: 0.875\n",
      "\n",
      "Exiting 14 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 7 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 9 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 4 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 5 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 13 afte n. steps 83. Tot reward: 1.0\n",
      "Exiting 10 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 12 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 0 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 3 afte n. steps 83. Tot reward: 0.0\n",
      "Exiting 2 afte n. steps 97. Tot reward: 1.0\n",
      "Exiting 15 afte n. steps 112. Tot reward: 2.0\n",
      "Exiting 8 afte n. steps 97. Tot reward: 1.0\n",
      "Exiting 1 afte n. steps 110. Tot reward: 2.0\n",
      "Exiting 11 afte n. steps 122. Tot reward: 2.0\n",
      "Exiting 6 afte n. steps 122. Tot reward: 2.0\n",
      "Epoch: 1 ended with average reward: 0.75\n",
      "\n",
      "Exiting 7 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 3 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 13 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 4 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 10 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 8 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 14 afte n. steps 74. Tot reward: 0.0\n",
      "Exiting 0 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 6 afte n. steps 97. Tot reward: 1.0\n",
      "Exiting 12 afte n. steps 84. Tot reward: 0.0\n",
      "Exiting 2 afte n. steps 74. Tot reward: 0.0\n",
      "Exiting 9 afte n. steps 84. Tot reward: 0.0\n",
      "Exiting 1 afte n. steps 83. Tot reward: 0.0\n",
      "Exiting 11 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 5 afte n. steps 89. Tot reward: 1.0\n",
      "Exiting 15 afte n. steps 120. Tot reward: 1.0\n",
      "Epoch: 2 ended with average reward: 0.3125\n",
      "\n",
      "Exiting 4 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 2 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 5 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 12 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 11 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 10 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 8 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 13 afte n. steps 74. Tot reward: 0.0\n",
      "Exiting 0 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 3 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 15 afte n. steps 83. Tot reward: 1.0\n",
      "Exiting 9 afte n. steps 83. Tot reward: 1.0\n",
      "Exiting 7 afte n. steps 90. Tot reward: 2.0\n",
      "Exiting 6 afte n. steps 114. Tot reward: 3.0\n",
      "Exiting 1 afte n. steps 99. Tot reward: 1.0\n",
      "Exiting 14 afte n. steps 123. Tot reward: 3.0\n",
      "Epoch: 3 ended with average reward: 0.9375\n",
      "\n",
      "Exiting 4 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 12 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 0 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 14 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 13 afte n. steps 74. Tot reward: 0.0\n",
      "Exiting 7 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 1 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 11 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 2 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 15 afte n. steps 83. Tot reward: 0.0\n",
      "Exiting 9 afte n. steps 108. Tot reward: 0.0\n",
      "Exiting 10 afte n. steps 89. Tot reward: 1.0\n",
      "Exiting 6 afte n. steps 112. Tot reward: 2.0\n",
      "Exiting 5 afte n. steps 122. Tot reward: 2.0\n",
      "Exiting 8 afte n. steps 123. Tot reward: 3.0\n",
      "Exiting 3 afte n. steps 123. Tot reward: 2.0\n",
      "Epoch: 4 ended with average reward: 0.75\n",
      "\n",
      "Exiting 1 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 13 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 7 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 12 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 3 afte n. steps 74. Tot reward: 0.0\n",
      "Exiting 2 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 10 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 11 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 8 afte n. steps 89. Tot reward: 1.0\n",
      "Exiting 9 afte n. steps 74. Tot reward: 0.0\n",
      "Exiting 5 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 14 afte n. steps 98. Tot reward: 1.0\n",
      "Exiting 0 afte n. steps 83. Tot reward: 1.0\n",
      "Exiting 6 afte n. steps 84. Tot reward: 1.0\n",
      "Exiting 15 afte n. steps 75. Tot reward: 1.0\n",
      "Exiting 4 afte n. steps 199. Tot reward: 4.0\n",
      "Epoch: 5 ended with average reward: 0.875\n",
      "\n",
      "Exiting 8 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 12 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 13 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 9 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 2 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 0 afte n. steps 74. Tot reward: 0.0\n",
      "Exiting 15 afte n. steps 89. Tot reward: 1.0\n",
      "Exiting 7 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 6 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 10 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 4 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 14 afte n. steps 75. Tot reward: 1.0\n",
      "Exiting 11 afte n. steps 98. Tot reward: 1.0\n",
      "Exiting 1 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 3 afte n. steps 98. Tot reward: 1.0\n",
      "Exiting 5 afte n. steps 124. Tot reward: 0.0\n",
      "Epoch: 6 ended with average reward: 0.4375\n",
      "\n",
      "Exiting 14 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 6 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 3 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 10 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 5 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 11 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 1 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 9 afte n. steps 74. Tot reward: 0.0\n",
      "Exiting 4 afte n. steps 89. Tot reward: 1.0\n",
      "Exiting 13 afte n. steps 84. Tot reward: 1.0\n",
      "Exiting 15 afte n. steps 74. Tot reward: 0.0\n",
      "Exiting 8 afte n. steps 89. Tot reward: 1.0\n",
      "Exiting 2 afte n. steps 88. Tot reward: 1.0\n",
      "Exiting 7 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 12 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 0 afte n. steps 155. Tot reward: 3.0\n",
      "Epoch: 7 ended with average reward: 0.5625\n",
      "\n",
      "Exiting 0 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 14 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 13 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 9 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 2 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 15 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 5 afte n. steps 74. Tot reward: 0.0\n",
      "Exiting 12 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 10 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 4 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 3 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 7 afte n. steps 99. Tot reward: 1.0\n",
      "Exiting 8 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 1 afte n. steps 99. Tot reward: 0.0\n",
      "Exiting 11 afte n. steps 98. Tot reward: 1.0\n",
      "Exiting 6 afte n. steps 170. Tot reward: 2.0\n",
      "Epoch: 8 ended with average reward: 0.75\n",
      "\n",
      "Exiting 4 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 1 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 7 afte n. steps 60. Tot reward: 0.0\n",
      "Exiting 0 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 6 afte n. steps 74. Tot reward: 0.0\n",
      "Exiting 12 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 8 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 11 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 5 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 2 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 15 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 9 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 13 afte n. steps 106. Tot reward: 1.0\n",
      "Exiting 3 afte n. steps 106. Tot reward: 1.0\n",
      "Exiting 14 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 10 afte n. steps 170. Tot reward: 2.0\n",
      "Epoch: 9 ended with average reward: 0.875\n",
      "\n",
      "Exiting 9 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 0 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 7 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 1 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 12 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 10 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 8 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 3 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 13 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 4 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 14 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 2 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 15 afte n. steps 106. Tot reward: 1.0\n",
      "Exiting 6 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 5 afte n. steps 106. Tot reward: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting 11 afte n. steps 106. Tot reward: 1.0\n",
      "Epoch: 10 ended with average reward: 1.0\n",
      "\n",
      "Exiting 1 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 2 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 14 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 0 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 11 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 3 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 13 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 12 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 8 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 6 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 7 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 10 afte n. steps 106. Tot reward: 1.0\n",
      "Exiting 4 afte n. steps 106. Tot reward: 1.0\n",
      "Exiting 9 afte n. steps 106. Tot reward: 1.0\n",
      "Exiting 5 afte n. steps 106. Tot reward: 1.0\n",
      "Exiting 15 afte n. steps 106. Tot reward: 1.0\n",
      "Epoch: 11 ended with average reward: 1.0\n",
      "\n",
      "Exiting 2 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 5 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 9 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 4 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 1 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 10 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 3 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 12 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 11 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 6 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 13 afte n. steps 74. Tot reward: 1.0\n",
      "Exiting 8 afte n. steps 106. Tot reward: 1.0\n",
      "Exiting 14 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 15 afte n. steps 106. Tot reward: 1.0\n",
      "Exiting 7 afte n. steps 90. Tot reward: 1.0\n",
      "Exiting 0 afte n. steps 106. Tot reward: 1.0\n",
      "Epoch: 12 ended with average reward: 1.0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m         highest_average_reward \u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     40\u001b[0m         ppo\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPISODES):\n\u001b[0;32m     45\u001b[0m     envs[i_env]\u001b[38;5;241m.\u001b[39mclose()\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size):\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_advantages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_steps):\n\u001b[0;32m     76\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(batch_size)\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mMemory.calculate_advantages\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m G\u001b[38;5;241m.\u001b[39mappend(trajectory[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREWARD] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39mppo\u001b[38;5;241m.\u001b[39mreturn_v_values(trajectory[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATE]))\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(trajectory)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 36\u001b[0m     delta\u001b[38;5;241m.\u001b[39mappend(trajectory[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREWARD] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39m\u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_v_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectory\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTATE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m ppo\u001b[38;5;241m.\u001b[39mreturn_v_values(trajectory[i][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATE]))\n\u001b[0;32m     37\u001b[0m     G\u001b[38;5;241m.\u001b[39mappend(trajectory[i][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREWARD] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39mG[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     38\u001b[0m     new_advantage \u001b[38;5;241m=\u001b[39m delta[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_p\u001b[38;5;241m*\u001b[39mold_advantage\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mPPO.return_v_values\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreturn_v_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[1;32m--> 133\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py:1720\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1714\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   1715\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing Model.predict with \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1716\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiWorkerDistributionStrategy or TPUStrategy and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1717\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoShardPolicy.FILE might lead to out-of-order result\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1718\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1720\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1723\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\keras\\engine\\data_adapter.py:1383\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1382\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\keras\\engine\\data_adapter.py:1138\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1135\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m   1137\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1152\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\keras\\engine\\data_adapter.py:322\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m    320\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[1;32m--> 322\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    325\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshuffle_batch\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch):\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\keras\\engine\\data_adapter.py:354\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m    352\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m d: tf\u001b[38;5;241m.\u001b[39mgather(d, i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), data)\n\u001b[1;32m--> 354\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrab_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[0;32m    359\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1863\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m MapDataset(\u001b[38;5;28mself\u001b[39m, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1863\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallelMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1866\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1867\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1868\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5020\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   5018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[0;32m   5019\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m-> 5020\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5022\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5024\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5026\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4218\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   4211\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   4212\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4213\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4214\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4215\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4216\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m-> 4218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4219\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m   4220\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3150\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   3142\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m   3143\u001b[0m \n\u001b[0;32m   3144\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3148\u001b[0m \u001b[38;5;124;03m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[0;32m   3149\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3150\u001b[0m   graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_concrete_function_garbage_collected(\n\u001b[0;32m   3151\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3152\u001b[0m   graph_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   3153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3116\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3114\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m-> 3116\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3117\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m   3118\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[0;32m   3119\u001b[0m       graph_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3463\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[0;32m   3460\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed\u001b[38;5;241m.\u001b[39madd(call_context_key)\n\u001b[1;32m-> 3463\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mprimary[cache_key] \u001b[38;5;241m=\u001b[39m graph_function\n\u001b[0;32m   3466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3298\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3293\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3294\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3295\u001b[0m ]\n\u001b[0;32m   3296\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   3297\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3298\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3300\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3301\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3303\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3306\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3307\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   3310\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   3311\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3312\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3313\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3314\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3315\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:923\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m    921\u001b[0m   arg_shapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    922\u001b[0m   kwarg_shapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 923\u001b[0m func_args \u001b[38;5;241m=\u001b[39m \u001b[43m_get_defun_inputs_from_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m func_kwargs \u001b[38;5;241m=\u001b[39m _get_defun_inputs_from_kwargs(\n\u001b[0;32m    926\u001b[0m     kwargs, flat_shapes\u001b[38;5;241m=\u001b[39mkwarg_shapes)\n\u001b[0;32m    928\u001b[0m \u001b[38;5;66;03m# Convert all Tensors into TensorSpecs before saving the structured inputs.\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;66;03m# If storing pure concrete functions that are not called through polymorphic\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# functions, we don't have access to FunctionSpec, so we need to call the\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;66;03m# TensorSpecs by their `arg_names` for later binding.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1159\u001b[0m, in \u001b[0;36m_get_defun_inputs_from_args\u001b[1;34m(args, names, flat_shapes)\u001b[0m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_defun_inputs_from_args\u001b[39m(args, names, flat_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1158\u001b[0m   \u001b[38;5;124;03m\"\"\"Maps Python function positional args to graph-construction inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1159\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_defun_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_shapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1232\u001b[0m, in \u001b[0;36m_get_defun_inputs\u001b[1;34m(args, names, structure, flat_shapes)\u001b[0m\n\u001b[0;32m   1230\u001b[0m placeholder_shape \u001b[38;5;241m=\u001b[39m shape \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1232\u001b[0m   placeholder \u001b[38;5;241m=\u001b[39m \u001b[43mgraph_placeholder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m      \u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplaceholder_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequested_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   1236\u001b[0m   \u001b[38;5;66;03m# Sometimes parameter names are not valid op names, so fall back to\u001b[39;00m\n\u001b[0;32m   1237\u001b[0m   \u001b[38;5;66;03m# unnamed placeholders.\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m   placeholder \u001b[38;5;241m=\u001b[39m graph_placeholder(arg\u001b[38;5;241m.\u001b[39mdtype, placeholder_shape)\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\eager\\graph_only_ops.py:38\u001b[0m, in \u001b[0;36mgraph_placeholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m     36\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[0;32m     37\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype_value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: shape}\n\u001b[1;32m---> 38\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlaceholder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m result, \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39moutputs\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_callbacks\u001b[38;5;241m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[0;32m     43\u001b[0m   \u001b[38;5;66;03m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[0;32m     44\u001b[0m   \u001b[38;5;66;03m# are unified. Remove this `if` block.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:599\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    597\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    598\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[1;32m--> 599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFuncGraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3561\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3558\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   3559\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   3560\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 3561\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3562\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3563\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3564\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3565\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3566\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3567\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3568\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3569\u001b[0m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3570\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   3571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2041\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   2039\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m op_def \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2040\u001b[0m     op_def \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39m_get_op_def(node_def\u001b[38;5;241m.\u001b[39mop)\n\u001b[1;32m-> 2041\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c_op \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2042\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcontrol_input_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2043\u001b[0m   name \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_traceback \u001b[38;5;241m=\u001b[39m tf_stack\u001b[38;5;241m.\u001b[39mextract_stack_for_node(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c_op)\n",
      "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1853\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1851\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _reconstruct_sequence_inputs(op_def, inputs, node_def\u001b[38;5;241m.\u001b[39mattr)\n\u001b[0;32m   1852\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m op_desc \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_NewOperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node_def\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[0;32m   1857\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetDevice(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mdevice))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "N_EPISODES = 16 # in multi-agent this is the number of agents (each agnet collect 1 trajectory)\n",
    "N_STEPS = 200 # max number of step for each episode\n",
    "\n",
    "TRAIN_STEPS = 50 # number of max steps done during training. if the number of samples is less than TRAIN_STEPS*BATCH_SIZE will stop early after completing the training on all the samples\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "#env used to initialize the parameters inside PPO and RND\n",
    "environment = \"BreakoutNoFrameskip-v4\" # \"AirRaidNoFrameskip-v4\", \"GravitarFrameskip-v4\"\n",
    "\n",
    "ppo = PPO(n_episodes = N_EPISODES, train_steps = TRAIN_STEPS)\n",
    "\n",
    "rewards = []\n",
    "\n",
    "envs = []\n",
    "\n",
    "for i_env in range(N_EPISODES):\n",
    "        basic_env = gym.make(environment, obs_type = \"image\")\n",
    "        wrapped_env = AtariPreprocessing(basic_env)\n",
    "        envs.append(FrameStack(wrapped_env, 4))\n",
    "\n",
    "highest_average_reward = 0\n",
    "        \n",
    "for i_epoch in range(N_EPOCHS):\n",
    "    epoch_reward = 0.0\n",
    "    agents = []\n",
    "    for i_agent in range(N_EPISODES):\n",
    "        agents.append(collect_trajectory(environment = environment, i_agent = i_agent, env = envs[i_agent]))\n",
    "    for agent in agents:\n",
    "        agent.start()\n",
    "    for agent in agents:\n",
    "        agent.join()\n",
    "        agent_reward = agent.get_reward()\n",
    "        epoch_reward = epoch_reward + agent_reward\n",
    "    rewards.append(epoch_reward/N_EPISODES)\n",
    "    print(\"Epoch: {} ended with average reward: {}\\n\".format(i_epoch, epoch_reward/N_EPISODES))  \n",
    "    \n",
    "    if (highest_average_reward <= rewards[-1]):\n",
    "        highest_average_reward = rewards[-1]\n",
    "        ppo.save()\n",
    "        \n",
    "    ppo.train(batch_size = BATCH_SIZE)\n",
    "    \n",
    "for i_env in range(N_EPISODES):\n",
    "    envs[i_env].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1ff4508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gym.wrappers.frame_stack.LazyFrames object at 0x000001BAB276F310>\n",
      "Starting demo\n",
      "step 0 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 1 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 2 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 3 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 4 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 5 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 6 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 7 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 8 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 9 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 10 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 11 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 12 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 13 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 14 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 15 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 16 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 17 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 18 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 19 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 20 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 21 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 22 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 23 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 24 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 25 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 26 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 27 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 28 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 29 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 30 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 31 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 32 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 33 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 34 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 35 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 36 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 37 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 38 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 39 selected action 3 with prob 1.0 got reward 1.0\n",
      "step 40 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 41 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 42 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 43 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 44 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 45 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 46 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 47 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 48 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 49 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 50 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 51 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 52 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 53 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 54 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 55 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 56 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 57 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 58 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 59 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 60 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 61 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 62 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 63 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 64 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 65 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 66 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 67 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 68 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 69 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 70 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 71 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 72 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 73 selected action 3 with prob 1.0 got reward 0.0\n",
      "step 74 selected action 3 with prob 1.0 got reward 0.0\n"
     ]
    }
   ],
   "source": [
    "basic_env = gym.make(\"BreakoutNoFrameskip-v4\", obs_type = \"image\")\n",
    "wrapped_env = AtariPreprocessing(basic_env)\n",
    "stack_env = FrameStack(wrapped_env, 4)\n",
    "\n",
    "#ppo = PPO()\n",
    "\n",
    "#ppo.load()\n",
    "\n",
    "observation = stack_env.reset()\n",
    "observation, reward, done, info = stack_env.step(1)\n",
    "\n",
    "past_lifes = info['ale.lives']\n",
    "\n",
    "print(observation)\n",
    "print(\"Starting demo\")\n",
    "for i_step in range(300):   \n",
    "    \n",
    "    action, action_prob = ppo.select_action(observation)\n",
    "    \n",
    "    observation, reward, done, info = stack_env.step(action)\n",
    "    \n",
    "    current_lifes = info['ale.lives']\n",
    "    \n",
    "    if (current_lifes <= past_lifes):\n",
    "        past_lifes = current_lifes\n",
    "        observation, reward, done, info = stack_env.step(1)\n",
    "    \n",
    "    #rand_action = stack_env.action_space.sample()\n",
    "    #observation, reward, done, info = stack_env.step(rand_action)\n",
    "    print(\"step {} selected action {} with prob {} got reward {}\".format(i_step, action, action_prob, reward))\n",
    "\n",
    "    stack_env.render()\n",
    "            \n",
    "    if (done):\n",
    "        break\n",
    "        \n",
    "stack_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5b122",
   "metadata": {},
   "source": [
    "## Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a82a0753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(13)#N_EPOCHS)\n",
    "\n",
    "plt.plot(epochs, rewards)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d710415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
